[
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.4-frontend/5.4.1-amazonamplify/",
	"title": "Amazon Amplify",
	"tags": [],
	"description": "",
	"content": "AMPLIFY 1. Method 1: Amplify Console (Recommended) Go to AWS Console → Amplify → Create new app Select Host web app Connect repository: GitHub/GitLab/Bitbucket ​​Authorize and select repo Build configuration: amplify.yml (already in the project):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment Variables: VITE_API_BASE_URL: API Gateway URL VITE_COGNITO_USER_POOL_ID: User Pool ID VITE_COGNITO_CLIENT_ID: Client ID VITE_COGNITO_REGION: ap-southeast-1 Deploy and get Amplify URL 2. Method 2: Amplify CLI # Setting npm install -g @aws-amplify/cli # Configure AWS credentials amplifier configure # Initialize in project cd serverless-student-management-system-front-end amplify init # Add hosting amplify add hosting # Choose: Hosting with Amplify Console # Select: Manual deployment.deployment # Deploy amplify publish "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.3-backend/5.3.1-amazondynamodb/",
	"title": "Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "1. Amazon DynamoDB DynamoDB is a serverless NoSQL database that stores all system data:\nStudent information Classes, subjects Lecturers Scores Spring Boot backend system interacts with DynamoDB via:\nAWS SDK for Java 17 Spring Data DynamoDB or repository written by the team Presigned URL (upload documents, profiles) 1.1 DynamoDB table design (Single-Table Design) 1.1.1 Create Tables Table 1: Users\nTable name: student-management-users\rPartition key: id (String)\rSort key: email (String) Table 2: Classes\nTable name: student-management-classes\rPartition key: id (String) Table 3: Subjects\nTable name: student-management-subjects\rPartition key: id (String) Table 4: Notifications\nTable name: student-management-notifications\rPartition key: id (Number)\rSort key: sent_at (String) 1.1.2 Global Secondary Index (GSI) Configuration For the Users table, add GSI:\nIndex name: role-index Partition key: role (String) "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Lam Thanh Phuc\nPhone Number: 0911797901\nEmail: phucltse184678@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam LLC\nInternship Position: FCJ Cloud Intern\nInternship Period: From 08/09/2025 to 09/12/2025\nReport Contents Worklog Proposal Translated Blogs Participated Events Workshop Self-evaluation Feedback \u0026amp; Suggestions "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Serverless Student Management System (SMS) is a modern student management platform, built entirely on AWS serverless services. This system helps schools or small businesses easily manage, analyze, and interact with student data on a small to large scale without worrying about hardware infrastructure. Serverless components such as Lambda, DynamoDB, API Gateway, Amplify, ROUTE53, CloudFront, WAF help the system automatically scale, optimize costs, maintain high reliability and security.\nUsers operate through the web interface (React/Amplify), interact with dashboards, assignments, and scoreboards.\nAPI backend (REST) ensures access to student data, personalized access.\nNo need to manage traditional servers, the entire deployment and operation process – from storage, processing, to CI/CD – is integrated and automated through AWS services.\nWorkshop overview In this workshop, you will:\nInitialize and configure AWS core serverless services: DynamoDB, Lambda, API Gateway, Amplify, Cognito.\nBuild and test a student management system with features: CRUD dashboard, service quality monitoring.\nExperience the DevOps CI/CD process automated from GitLab to AWS Pipeline, test and demo live the system.\nApply practical knowledge to real lessons or similar projects, easily expand to mobile environments or integrate advanced AI.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Worklog Overview During my 12-week internship at First Cloud Journey (from 08/09/2025 to 09/12/2025), I completed the AWS learning program and developed the Serverless Student Management System project.\nPhase 1: Learning AWS Fundamentals (Weeks 1-7) In the first 7 weeks, I focused on learning and practicing core AWS services:\nWeek Topic Skills Acquired 1 AWS Fundamentals Understanding Cloud Computing, AWS Global Infrastructure 2 IAM \u0026amp; Security Creating Users, Groups, Roles, MFA, Budget Alerts 3 VPC Basics Designing Subnets, Route Tables, IGW, NAT Gateway 4 VPC Security Configuring Security Groups, NACLs, VPC Peering 5 Hybrid Networking Transit Gateway, Route 53 Resolver, Hybrid DNS 6 EC2 Instance Types, AMI, EBS, Auto Scaling Groups 7 Storage AWS Backup, Storage Gateway, S3 Static Website Phase 2: Developing Proposal Backend (Weeks 8-12) In the last 5 weeks, I developed Backend APIs for the student management system:\nWeek Task Result 8 Analysis \u0026amp; Design Architecture diagram, API design, DynamoDB schema 9 Class \u0026amp; Student APIs 5 APIs: CRUD Class, List Students 10 Assignment \u0026amp; Grade APIs 5 APIs: CRUD Assignment, Update Grades 11 Post API \u0026amp; Testing 1 API, Integration tests, API documentation 12 Deploy \u0026amp; Demo Deploy AWS, Proposal document, Presentation Weekly Details Week 1: Getting Started with AWS - Cloud Computing and AWS Global Infrastructure\nWeek 2: Setting Up AWS Account - IAM, MFA, Budget, and Support\nWeek 3: Amazon VPC Basics - Subnets, Route Tables, Internet Gateway, NAT Gateway\nWeek 4: VPC Security and Advanced Connectivity - Security Groups, NACLs, VPC Peering\nWeek 5: AWS Transit Gateway and Hybrid DNS with Route 53 Resolver\nWeek 6: Amazon EC2 - Instance Types, AMI, EBS, Auto Scaling\nWeek 7: AWS Backup, Storage Gateway, and Amazon S3 Static Website\nWeek 8: Proposal - Requirement Analysis, Architecture Design, and API Class Management\nWeek 9: Proposal - Developing APIs: Create/List/Edit/Deactivate Class, List Students\nWeek 10: Proposal - Developing APIs: Assignment CRUD, Create/Update Grade\nWeek 11: Proposal - Developing Post/Comment API, Integration Testing, API Documentation\nWeek 12: Proposal - Deploy to AWS, Finalize Proposal, Prepare Demo \u0026amp; Presentation\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Introduce overview of AI/ML landscape in Vietnam Guide on using Amazon SageMaker for end-to-end ML platform Practice with Generative AI using Amazon Bedrock Learn about Prompt Engineering and RAG architecture Build GenAI chatbot with Bedrock Key Highlights 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking: Registration and connecting participants Workshop overview and learning objectives: Workshop overview and learning goals Ice-breaker activity: Ice-breaking activities Overview of the AI/ML landscape in Vietnam: Overview of AI/ML field in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform\nData preparation and labeling: Preparing and labeling data Model training, tuning, and deployment: Training, optimizing and deploying models Integrated MLOps capabilities: Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough - Live demo of SageMaker Studio 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models\nClaude, Llama, Titan: Comparison and selection guide for appropriate models Prompt Engineering\nTechniques: Prompt engineering techniques Chain-of-Thought reasoning: Chain-of-thought reasoning Few-shot learning: Learning with few examples Retrieval-Augmented Generation (RAG)\nArchitecture: RAG architecture Knowledge Base integration: Knowledge Base integration Bedrock Agents\nMulti-step workflows: Multi-step workflows Tool integrations: Tool integrations Guardrails\nSafety and content filtering: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock\nKey Takeaways AI/ML Fundamentals AI/ML landscape: Understanding AI/ML situation in Vietnam and globally Machine Learning workflow: Process from data preparation to model deployment MLOps practices: Best practices for operating ML models in production Amazon SageMaker End-to-end ML platform: Comprehensive ML platform from data to deployment Data preparation: Techniques for preparing and labeling data effectively Model training \u0026amp; tuning: Training and optimizing models Deployment strategies: Strategies for deploying models to production MLOps integration: Integrating MLOps for continuous training and monitoring Generative AI with Bedrock Foundation Models: Understanding differences between Claude, Llama, Titan Model selection: Criteria for choosing appropriate model for each use case Prompt Engineering: Techniques for writing effective prompts Chain-of-Thought reasoning for complex tasks Few-shot learning to improve output RAG Architecture: Retrieval-Augmented Generation for accurate responses Knowledge Base integration Vector databases and semantic search Bedrock Agents: Multi-step workflows automation Tool integrations for extended capabilities Guardrails: Safety and content filtering mechanisms Applying to Work Implement SageMaker: Deploy ML models for business use cases Build GenAI applications: Build GenAI applications with Bedrock Apply Prompt Engineering: Use prompt engineering techniques in projects Integrate RAG: Implement RAG architecture for knowledge-based applications MLOps adoption: Apply MLOps practices for ML lifecycle management Experiment with Foundation Models: Experiment with different foundation models Event Experience Attending the \u0026ldquo;AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\u0026rdquo; workshop was an exciting and knowledge-filled morning, giving me a comprehensive view of AI/ML and Generative AI on AWS. Key experiences included:\nLearning from AI/ML experts Speakers from AWS shared best practices in building and deploying ML solutions. Through real demos of SageMaker Studio, I better understood the ML workflow from data preparation to deployment. Generative AI experience Participating in Amazon Bedrock sessions helped me understand foundation models and how to use them. Learned important prompt engineering techniques like Chain-of-Thought and Few-shot learning. Practiced building GenAI chatbot with Bedrock and understood RAG architecture. AI/ML Landscape Understood the AI/ML landscape in Vietnam and opportunities. Learned about real use cases of AI/ML in enterprises. Networked with AI/ML community in Vietnam. Networking and discussions Workshop provided opportunities to exchange with AI/ML practitioners and solution architects. Discussed challenges and best practices in implementing AI/ML solutions. Shared experiences about prompt engineering and model selection. Lessons learned AI/ML democratization: AWS services help democratize AI/ML for all developers. Prompt Engineering is critical: Quality of prompts directly affects output quality. RAG enhances accuracy: Retrieval-Augmented Generation improves GenAI accuracy. MLOps is essential: Need MLOps to operate ML models effectively in production. Foundation Models selection: Choosing the right foundation model is a key decision. Overall, the event not only provided AI/ML knowledge but also helped me understand how to apply these technologies to real-world applications, opening up many possibilities for innovation.\nDesign Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect, get acquainted, and form a group with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit 06/09/2025 08/09/2025 3 - Study theory modules 01-01 to 01-04 + AWS vision, culture, leadership principles + Basic theory of cloud computing + AWS global infrastructure (Concepts of Data Center, Availability Zone, Region, Edge Locations) 09/09/2025 09/09/2025 https://www.youtube.com/@AWSStudyGroup https://cloudjourney.awsstudygroup.com/ 4 - Study theory modules 01-05 to 01-06 - AWS Service management tools (Concepts of Root user, IAM user, how to use AWS services via AWS Management Console, CLI, and AWS SDK) - Cost optimization methods on AWS: + Reserved instance + Saving plan + Spot - Learn about Serverless concepts, AWS budget, cost allocation tag, estimate - Learn about 4 AWS Support packages (Basic, Developer, Business, Enterprise) 10/09/2025 10/0098/2025 https://www.youtube.com/@AWSStudyGroup https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create AWS account + Set up MFA + Create Admin Group and Admin user + Create Access key (CLI) + Create Budget + Create Cost Budget 11/09/2025 11/09/2025 https://www.youtube.com/@AWSStudyGroup https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Learn how to create Usage Budget + Learn how to create RI Budget + Learn how to create Saving Plan Budget + How to delete Budget (release resources) + How to use AWS Support 12/09/2025 12/09/2025 https://www.youtube.com/@AWSStudyGroup https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Got acquainted and connected with First Cloud Journey members, understood the rules and regulations at the internship unit.\nUnderstood basic AWS concepts:\nAWS vision, culture, and leadership principles Basic theory of cloud computing AWS global infrastructure (Data Center, Availability Zone, Region, Edge Locations) Mastered AWS Service management tools:\nUnderstood the concept of Root user and IAM user Understood how to use AWS Management Console and CLI Understood AWS cost optimization methods:\nReserved instance Saving plan Spot Serverless concepts, AWS budget, cost allocation tag, estimate Clearly understood the 4 AWS Support packages (Basic, Developer, Business, Enterprise)\nPracticed the following skills:\nCreated AWS account and set up security with MFA Created Admin Group and Admin user Created Access key for CLI Set up and managed various Budgets (Cost Budget, Usage Budget, RI Budget, Saving Plan Budget) Used AWS Support Successfully configured AWS Free Tier account and learned how to optimize to avoid unnecessary costs.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1 - AWS Fault Injection Service",
	"tags": [],
	"description": "",
	"content": "Simulating Partial Failures with AWS Fault Injection Service Written by Ozgur Canibeyaz and Pablo Colazurdo | June 30, 2025 | in AWS Fault Injection Service (FIS), AWS Resilience Hub (ARH), AWS Systems Manager, Management Tools, Resilience, Technical How-to | Permalink | Share\nA modern distributed system must be able to withstand unexpected disruptions to maintain availability, performance, and stability. Chaos engineering helps teams uncover hidden weaknesses by intentionally injecting faults into the system and observing how it recovers. While traditional testing validates expected behavior, chaos engineering tests the system’s fault tolerance under failure conditions. AWS Fault Injection Service (AWS FIS) is a fully managed AWS service that helps teams run fault injection experiments on AWS workloads. It supports scenarios such as terminating Amazon EC2 instances, throttling Amazon API Gateway requests, and inducing network latency. This allows you to verify fault tolerance in a production-like environment. While these capabilities are powerful, many real-world failures only affect a portion of traffic.\nIn this article, you’ll learn how to simulate partial failures—a common but rarely tested failure mode—by combining AWS FIS with weighted routing in Application Load Balancer (ALB) and a AWS Lambda function that returns custom error responses. This approach lets you test how your application handles degraded conditions without changing code or disrupting normal traffic.\nOverview of Partial Failures Our solution combines AWS FIS with weighted routing on ALB to direct a configurable percentage of traffic to a Lambda function that returns simulated errors. This approach requires no application code changes and automatically reverts to normal operation after the experiment ends.\nFigure 1 illustrates how the solution modifies the Load Balancer to inject faults during the experiment and automatically restores it afterward.\nKey Benefits This solution provides key benefits for teams implementing chaos engineering:\nControlled fault simulation. No application changes required. Automated setup and rollback. Configurable error rate. Deployment Guide Prerequisites Before you begin, make sure you have:\nAn AWS account with permissions to deploy AWS CloudFormation stacks and manage AWS FIS experiments. An ALB configured with a target group routing traffic to a running microservice. ALB must be active and publicly accessible to test simulated errors. AWS Command Line Interface (AWS CLI) or access to the AWS Management Console. Step 1: Deploy the CloudFormation Template The CloudFormation template sets up all required resources, including:\nA Lambda function to simulate error responses. An automation document for AWS Systems Manager (SSM). An IAM role granting AWS FIS permission to invoke the SSM automation document. An AWS FIS experiment template. Figure 2 shows an overview of the solution components and their interactions.\nConfigurable Experiment Parameters The CloudFormation template requires three parameters at deployment:\nName of the Application Load Balancer. ARN of the ALB listener rule to modify. Experiment duration in seconds—the time during which partial failure is simulated. Other experiment settings, such as the percentage of redirected traffic and Lambda response code, are preconfigured in the experiment definition. If you want to customize these values, you have two options:\nOption 1: Edit the CloudFormation Template and Redeploy\nYou can edit the documentParameters field in the experiment definition to change:\nFailurePercentage (e.g., 10, 50, 100). To change the Lambda function’s HTTP response code (e.g., from 500 to 404), edit the statusCode value directly in the inline code section of the template.\nAfter editing, redeploy the stack to apply changes.\nOption 2: Create a New Version of the SSM Automation Document\nIf you don’t want to redeploy the entire stack:\nGo to AWS Systems Manager → Documents console. Find the SSM document created by the template. Select Create new version and adjust default values such as FailurePercentage. Use the updated version in a new AWS FIS experiment (via CLI or console). IAM Permissions:\nYou need permissions to create IAM roles and policies when deploying the CloudFormation template. When deploying via AWS Console, confirm that the template creates IAM resources. If using AWS CLI, add the \u0026ndash;capabilities CAPABILITY_NAMED_IAM flag.\nDownload the template: You can download the CloudFormation template here and save it as fis_template.yaml before deploying via AWS Console or CLI.\naws cloudformation create-stack --stack-name alb-fis-experiment \\ --template-body file://fis_template.yaml \\ --parameters \\ ParameterKey=LoadBalancerName,ParameterValue=LoadBalancerName \\ ParameterKey=ListenerRuleArn,ParameterValue=RuleARN \\ ParameterKey=TestDurationInSeconds,ParameterValue=60 \\ --capabilities CAPABILITY_NAMED_IAM LoadBalancerName and RuleARN correspond to the Load Balancer name and full ARN of the listener rule before the service you want to simulate errors for. The value 60 specifies the experiment duration in seconds.\nNote: The FISExperimentRole IAM policy uses \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;*\u0026quot; for some actions so AWS FIS can dynamically modify load balancer resources. Since target group resource ARNs are not known before deployment, restricting permissions to specific resources is not feasible in this example. While this approach provides flexibility, AWS security best practices recommend limiting permissions to specific resources if you know which resources will be used. If you know exactly which resources will be used, consider updating the policy to restrict access accordingly.\nStep 2: Verify the Lambda Function After deployment, check the Lambda function in the AWS console to confirm it returns the expected error response. The function should return:\n{ \u0026#34;statusCode\u0026#34;: 503, \u0026#34;body\u0026#34;: \u0026#34;Service Unavailable - Simulated Error Response\u0026#34; } Step 3: Start the AWS FIS Experiment Open the AWS Fault Injection Service console. Find the preconfigured template in Experiment Templates. Select Start experiment. Confirm and launch the experiment. Figure 3 shows the AWS FIS console with a custom experiment template created by the CloudFormation template.\nWhen you start the experiment, AWS FIS invokes the AWS Systems Manager Automation Document created during deployment. This document performs the following actions:\nCreates a new ALB target group pointing to the Lambda function configured to return simulated error responses. Modifies the ALB listener rule to split a portion of traffic to the new target group, simulating partial failure. Waits for the configured duration (set via the CloudFormation template). Reverts the ALB listener rule to its original state and deletes the temporary target group. The entire lifecycle is automated—you don’t need to write code or manually update the load balancer. Just start the experiment from the FIS console and observe how your service responds to controlled failure scenarios.\nIn the screenshot below, you’ll see the initial ALB listener rule with only the default target group configured.\nFigure 4 shows the ALB listener rule before the experiment starts, with one target group receiving 100% of traffic.\nAfter the experiment starts, AWS FIS modifies the rule to split traffic—as shown in the next screenshot.\nFigure 5 shows the ALB listener rule after the experiment starts, with the new Lambda target group receiving 50% of traffic and returning the configured error response.\nStep 4: Observe and Analyze Results You can verify the experiment by refreshing the ALB DNS in your browser or running a curl loop:\nwhile true; do curl -s http://\u0026lt;your-alb-dns-name\u0026gt;; sleep 1; done Figure 6 shows CLI output sending continuous requests to the ALB URL to demonstrate fault injection during the experiment.\nYou’ll see alternating output between:\nBackend service is healthy (backend service) Service Unavailable – Simulated Error Response (Lambda) You can monitor Amazon CloudWatch Logs for Lambda metrics and Application behavior (retry, failover, etc.).\nNote: After starting the experiment, it may take up to a minute for the new target group to attach to the ALB and for traffic to begin routing to Lambda. During this brief period, all requests may still reach the original backend service.\nRollback Mechanism The experiment’s rollback mechanism:\nALB listener rule is automatically reverted when the experiment duration ends. Temporary target group is removed and deleted to avoid lingering configuration. If the experiment is canceled, the rollback process still returns the system to its original state. Considerations This article provides technical information and example configuration. Real-world deployments may require additional consideration for security, compliance, and technical factors. Always test thoroughly in non-production environments first.\nCleanup To avoid ongoing costs, delete deployed resources:\naws cloudformation delete-stack --stack-name alb-fis-experiment Conclusion In this article, we demonstrated how to extend AWS FIS capabilities by simulating partial failures for workloads behind ALB using Lambda. This solution lets teams test application fault tolerance against disruptions without causing a full outage. By leveraging AWS FIS, Lambda, and ALB routing rules, you can create controlled failure scenarios and improve system resilience.\nTo learn more, see:\nAWS Fault Injection Service Documentation Amazon ALB Documentation Use Systems Manager SSM documents with AWS FIS Get started with the CloudFormation template and share your experience in the comments below.\nTAGS: aws fault injection simulator, chaos engineering\nOzgur Canibeyaz Ozgur is a Senior Technical Account Manager at Amazon Web Services with 8 years of experience. Ozgur helps customers optimize their AWS usage by solving technical challenges, discovering cost-saving opportunities, achieving operational excellence, and building innovative services with AWS products.\nPablo Colazurdo Pablo is a Principal Solutions Architect at AWS where he enjoys helping customers launch successful projects on the Cloud. He has many years of experience working with diverse technologies and is always passionate about learning new things. Pablo grew up in Argentina but now enjoys the rain in Ireland while listening to music, reading books, or playing D\u0026amp;D with his kids.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.4-frontend/5.4.2-cloudfrontwaf/",
	"title": "Amazon CloudFornt &amp; WAF",
	"tags": [],
	"description": "",
	"content": "CLOUDFRONT + WAF 1. Create WAF Web ACL Go to AWS Console → WAF \u0026amp; Shield → Create web ACL Configuration: Name: student-management-waf Resource type: CloudFront distributions Region: Global (CloudFront) Add rules: AWS Managed Rules:\nAWSManagedRulesCommonRuleSet (Core rule set) AWSManagedRulesKnownBadInputsRuleSet AWSManagedRulesSQLiRuleSet (SQL injection) Rate limiting:\nName: RateLimitRule Rate limit: 2000 requests per 5 minutes per IP Rate limiting: Name: RateLimitRule Rate limit: 2000 requests per 5 minutes per IP 2. Create CloudFront Distribution Go to AWS Console → CloudFront → Create distribution\nOrigin Settings:\nOrigin domain: Amplify app URL (xxx.amplifyapp.com) Protocol: HTTPS only Default Cache Behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE Cache policy: CachingOptimized Origin request policy: AllViewer Settings:\nPrice class: Use all edge locations WAF web ACL: Choose the created ACL SSL certificate: Default CloudFront certificate (hoặc custom) Add Origin for API Gateway:\nAdd origin: API Gateway URL Create behavior: /api/* → API Gateway origin "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.3-backend/5.3.2-amazoncognito/",
	"title": "Amazon Cognito",
	"tags": [],
	"description": "",
	"content": "Amazon Cognito 1. Create User Pool Go to AWS Console → Cognito → Create user pool Configuration: Sign-in: Email Password policy: Minimum 8 characters MFA: Optional Email: Send email with Cognito Create App Clients: App client name: student-management-app Generate client secret: No Auth flows: ALLOW_USER_SRP_AUTH, ALLOW_REFRESH_TOKEN_AUTH 2. Save information VITE_COGNITO_USER_POOL_ID=ap-southeast-1_XXXXXXXX VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxx VITE_COGNITO_REGION=ap-southeast-1 "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "To successfully deploy the Serverless Student Management System Workshop, it is necessary to fully prepare the technical environment, AWS account and platform services according to the serverless – event-driven architecture.\nThis page guides the essential steps before starting to develop the backend, frontend and event components.\nPrepare AWS account Create and configure AWS account Create a personal AWS account or use AWS Educate/AWS Academy.\nActivate AWS Free Tier to optimize costs during the workshop period.\nEnable MFA to secure the root account.\nCreate IAM User for team members and assign roles according to the Least Privilege principle.\nFigure 1: Sample IAM users.\rSet up IAM Permissions To fully deploy the project\u0026rsquo;s serverless system, the IAM User needs permission to operate with:\nAWS Lambda DynamoDB API Gateway Cognito S3 CloudWatch Amplify IAM (limited to PassRole and Lambda role creation operations) An example of a broad permission policy for workshop purposes:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ServerlessStudentManagementWorkshopAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;lambda:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34;, \u0026#34;route53:*\u0026#34;, \u0026#34;acm:*\u0026#34;, \u0026#34;waf:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34;, \u0026#34;amplify:*\u0026#34;, \u0026#34;codebuild:*\u0026#34;, \u0026#34;codedeploy:*\u0026#34;, \u0026#34;codepipeline:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Serverless Student Management System This is the DOCX version of the proposal. Download the Proposal to view more details.\n1. Executive Summary Serverless Student Management Platform is a cloud-based student management platform designed for educational institutions and small businesses to enhance the ability to manage, analyze, and interact with student data. The platform supports up to 50-100 students initially, with the ability to flexibly scale up to 500–1000 students without major infrastructure changes, using AWS Serverless services such as Lambda, DynamoDB, API Gateway.\n2. Problem Statement Current Problem\nCurrent student management systems require manual data entry, which is difficult to manage when there are many classes. There is no centralized system for real-time data or analytics, and third-party platforms are often expensive and too complex.\nSolution\nThe platform uses Amazon API Gateway to receive REST requests, AWS Lambda to handle business logic, Amazon DynamoDB to store student data and scores. AWS Amplify with React/TypeScript provides the web interface, and Amazon Cognito ensures secure access. Similar to traditional LMS systems but at a lower cost, users can register new students and manage information, but the platform operates at a smaller scale and serves learning purposes.\nBenefits and ROI\nThe solution creates a basic foundation for IT students to develop serverless AWS skills, while providing effective management tools for teachers to support teaching and assessment. The platform reduces manual reporting for each class through a centralized system, simplifies management and maintenance, and improves data reliability. The estimated monthly cost is $7-20 USD according to the detailed budget estimate, totaling $40-60 USD for 3 months.\n3. Solution Architecture The system is designed according to the AWS Well-Architected Framework architecture with interconnected layers, ensuring data management, authentication, and monitoring capabilities. The serverless architecture helps optimize costs and ensure automatic scalability.\nAWS Core Services Used Services Key Features Key Benefits Amazon Route 53 Manage DNS and route traffic to CloudFront Set up custom domains, health checks, geo-routing, SSL integration to ensure secure and fast access. Amazon CloudFront CDN delivery for static content and frontend resources Global latency reduction, caching at edge locations, HTTPS support, WAF integration for protection and OAC for S3. AWS WAF Firewall to protect web applications from attacks Block malicious requests, rate limiting, IP filtering, seamless integration with CloudFront to secure traffic. AWS Amplify Frontend hosting and deployment with CI/CD Build and deploy web applications quickly, integrate with Cognito/AppSync for user-friendly interface. Amazon API Gateway Handle and route API requests from frontend to backend Support REST/HTTP APIs, throttling, caching, and authorizer for increased performance and security. Amazon Cognito Manage user authentication and authorization Support MFA, JWT tokens, groups for permissions (teacher/admin vs. student), easy integration with AppSync/API Gateway. AWS Lambda Execute backend logic and event handling Serverless, auto-scaling, pay-per-use, CRUD processing. Amazon DynamoDB NoSQL data storage for student information and assignments Fast query, auto-scaling, Global Secondary Indexes (GSI) support for complex searches and low cost. Amazon CloudWatch Monitor logs, metrics, and system alarms Real-time monitoring, set alarms to detect problems early, integrate with Lambda/DynamoDB for optimization. Amazon S3 Store artifacts and builds from CI/CD Cheap, highly sustainable static file hosting, integrated with CodePipeline to store deployed artifacts. GitLab Manage source code and trigger CI/CD pipeline Version control (GitLab.com or self-hosted), merge requests, issue tracking, webhooks integrated with CodePipeline to automate deployment. AWS CodePipeline Orchestrate CI/CD pipeline from source to deploy Automate the entire process from GitLab to production, integrated with CodeBuild/CodeDeploy, support approvals and notifications. AWS CodeBuild Build and test code from GitLab repository Automatically compile, package artifacts, run unit tests in pipeline, support multi-environment builds. AWS CodeDeploy Deploy applications to AWS services Support blue/green deployment, automatic rollback, zero-downtime deployments for Lambda/Amplify. Component Design Layer Main Components Functionality Edge Layer Route53, CloudFront, WAF DNS, CDN, security layer Frontend Layer Amplify, Cognito Web interface, authentication Backend Layer API Gateway, Lambda, DynamoDB Logic processing, CRUD, data storage Monitoring Layer CloudWatch Logs, metrics, alerts CI/CD Layer GitLab, CodePipeline, CodeBuild, CodeDeploy, S3 Build \u0026amp; deploy automation Main Flow Annotation Flow Description (1) User accesses via Route53 → CloudFront → Amplify (frontend). (2) Amplify communicates with API Gateway to send/receive data. (3.1) API Gateway → Lambda to handle logic (CRUD, etc.). (3.2) Cognito authenticates and returns tokens. (4) Dev pushes code to GitLab → CodePipeline pull → CodeBuild/CodeDeploy → Amplify/Lambda. (5) CloudWatch collects logs and metrics. 4. Technical Implementation Implementation Phases The project was implemented in 5 weeks with 5 main phases:\nWeek 1 - Foundation Setup Days 1-2: Set up AWS account, configure IAM roles/policies, set up billing alerts Days 3-4: Create DynamoDB tables (Students, Courses, Grades, Assignments) with GSI, configure Cognito User Pools with groups (Admin/Teacher/Student) Days 5-7: Initialize IaC templates (AWS CDK/CloudFormation), research serverless patterns, design detailed architecture with sequence diagrams, setup Git repo structure for parallel development Week 2 - Backend \u0026amp; Frontend Parallel (Parallel Development Phase 1) Backend Team (Day 1-7): Build 50+ Lambda functions for: Students (CRUD, search, bulk import/export), Courses (CRUD, enrollment), Grades (CRUD, analytics, statistics), Assignments (CRUD, submissions), Auth (login, registration, refresh token) Set up API Gateway with 50+ REST endpoints: /students/* (10 endpoints), /courses/* (8 endpoints), /grades/* (12 endpoints), /assignments/* (8 endpoints) Unit testing with 80%+ coverage Frontend Team (July 1): Deploy Amplify hosting with React/TypeScript, setup routing (React Router) Build UI components: Layout (Header, Sidebar, Footer), Authentication (Login, Register), Dashboard (Overview, Stats cards) Setup API client (Axios/Fetch), state management (Redux/Zustand), form validation (React Hook Form) Week 3 - Backend \u0026amp; Frontend Parallel (Parallel Development Phase 2) Backend Team (Day 1-7): Integrate Lambda handlers for automated workflows Integration testing with Postman/SWAGGER Frontend Team (July 1): Build 15+ pages: Student Management (List, Create, Edit, Detail, Import), Course Management (List, Create, Edit, Enrollment), Grade Management (List, Input, Analytics), Assignment (List, Submit, Review) Integration (Days 6-7): Connect CloudFront CDN with Route53, configure WAF rules (rate limiting, geo-blocking), SSL/TLS certificates End-to-end testing between Frontend and Backend Week 4 - CI/CD with CodeBuild, CodeDeploy, CodePipeline Day 1-2: Learn and configure AWS CodeBuild for frontend and backend (buildspec, build environment, save artifacts to S3) Day 3-4: Set up AWS CodePipeline to automate the build and deploy process from GitLab to AWS environment (connect stages: source, build, deploy) Day 5-6: Configure AWS CodeDeploy to deploy backend (Lambda or EC2) and frontend (Amplify or S3), test the automated deployment process Day 7: Test the entire pipeline, ensure the build, automated deployment works stably, record results and optimize the pipeline Week 5 - Testing \u0026amp; Demo (Testing \u0026amp; Launch) Day 1-2: End-to-end testing (user flows, edge cases), load testing with 100+ concurrent users Day 3-4: Performance optimization (Lambda memory tuning, DynamoDB capacity adjustment, CloudFront caching) Day 5-6: Complete documentation (architecture diagrams, API docs, deployment guide, user manual) Day 7: Demo preparation, presentation slides, video recording Technical Requirements Student Management System: Full web dashboard with 5 main modules (Students, Courses, Grades, Assignments). Frontend React/TypeScript running on Amplify Hosting with 15+ pages and 50+ components. Cognito authenticates and authorizes all users, including 5-10 admins/teachers (with high permissions like CRUD data) and students (with limited permissions like viewing scores, classes).\nComprehensive API architecture: 50+ REST API endpoints via API Gateway (CRUD operations, search, bulk actions). Backend built in parallel with Frontend to optimize development time.\nSmart Analytics Platform: Practical knowledge of AWS Amplify (hosting React), Lambda (50+ business processing functions), DynamoDB (5 tables with GSI). Use AWS CDK/SDK for IaC programming and automation. Parallel development methodology with Git branching strategy for Backend/Frontend teams.\n5. Roadmap \u0026amp; Milestones To fit the internship time, the project is implemented in 5 weeks with 5 main phases:\nPhase Time Main Objective Deliverables Success Criteria 1: Setting up the foundation Week 1 Setting up the AWS environment • AWS account with IAM setup\n• 7 DynamoDB tables with GSI\n• Cognito User Pool\n• IaC templates (CDK/CloudFormation)\n• Git repo structure • Infrastructure as Code complete\n• Security baseline up to standard\n• Parallel dev environment ready 2: Backend \u0026amp; Frontend Parallel (Phase 1) Week 2 Build core Backend + Frontend foundation • 50+ Lambda functions\n• API Gateway with 50+ REST endpoints\n• React/TypeScript app foundation\n• 10+ UI components\n• Unit tests (\u0026gt;80% coverage) • 50+ active API endpoints\n• API response time \u0026lt;500ms\n• Frontend routing setup\n• All backend tests passed 3: Backend \u0026amp; Frontend Parallel (Phase 2) Week 3 Integrating Lambda workflows \u0026amp; integration testing • Integrating Lambda handlers for automated workflows\n• Integration testing with Postman/SWAGGER\n• 15+ complete pages\n• CloudFront + Route53 + WAF\n• Responsive design • Automated workflows work well\n• Integration tested with Postman/SWAGGER\n• All pages integrated with backend\n• SSL/HTTPS enabled\n• Mobile responsive 4: CI/CD with CodeBuild, CodeDeploy, CodePipeline Week 4 Set up and test automated CI/CD process • Configure CodeBuild for frontend/backend\n• Set up CodePipeline connecting GitLab, CodeBuild, CodeDeploy\n• Test automated deployment for backend/frontend • Build and deploy automation work well\n• Artifacts stored correctly\n• Pipeline optimized 5: Test \u0026amp; Demo Week 5 Test and finalize • Load test reports (50+ users)\n• Performance optimization\n• Complete documentation\n• Demo video + slides • System uptime ≥99%\n• All features stable\n• Documentation complete\n• Demo ready 6. Budget Estimate Download budget estimate file for detailed cost information.\nInfrastructure Costs AWS Services:\nServices Usage Description Estimated Cost / Month (USD) Notes Amazon API Gateway Handles ~100k-500k API calls/month $3.8 HTTP APIs, REST APIs, first 1M calls free AWS Lambda ~200k-500k requests, 100k-200k GB-sec $0.5 Requests, GB-sec, 1M requests + 400k GB-sec free Amazon DynamoDB ~5-10 GB storage, 250k-500k reads/writes $1 Reads, Writes, Storage, 25 GB + 25 RCU/WCU free Amazon Cognito ~100-200 monthly active users $0 First 10k users free Amazon S3 ~5 GB storage, low requirements $0.13 Storage, requests, free credits available Amazon CloudWatch ~5 GB logs, 10 metrics/alarms $4 Logs, metrics, alarms, 5 GB free AWS Amplify Dashboard hosting, few builds/month $1 Build minutes, storage, free credits available Amazon Route 53 DNS queries, storage zones $1 Storage zone, DNS queries AWS WAF Web ACL Assessment, Rules $7 Web ACL, rules, rule management costs extra AWS CodeBuild ~10 builds/month, 100 minutes $0.9 $0.005/min build, 100 minutes/month free AWS CodeDeploy Application Deployment $0.8 EC2/on-premises, serverless free Amazon CloudFront Static Content Delivery CDN $1 First 1 TB/month free, $0.085/GB thereafter AWS CodePipeline CI/CD automation (GitLab → CodeBuild → CodeDeploy) $1 $1/month for each active pipeline Estimated total Total cost / month (estimated) 3-month total (estimated) With AWS Free Tier Notes ~$7 – $20 / month ~$40 – $60 / 3 months ~$15 – $30 / 3 months Depends on actual usage. Take advantage of Free Tier (first year) + AWS Educate credits to reduce costs by 50%. Note All core services are included in the first year free plan. Additional discounts may apply if using AWS Educate / Student Credits. 7. Risk Assessment Based on the NIST Risk Management Framework, the project team identified key risks and mitigation measures.\nRisk Code Description Level Mitigation R1 – Data Leakage Data exposure due to incorrect configuration High Apply Cognito auth, IAM least privilege, DynamoDB encryption R2 – API Overload Too many requests causing slowness Medium Throttling API Gateway/AppSync, CloudWatch alarms R3 – Lambda Cold Start Delay when invoking Medium Optimize code, Provisioned Concurrency if needed R4 – Cost Overrun Unusual usage increase (high emails) Medium AWS Budgets alerts, monitor Cost Explorer R5 – Service Downtime AWS Outage Low Multi-AZ config, backups DynamoDB Contingency Plan (Summary):\nRecovery: Use CloudFormation to quickly recreate infrastructure in the event of a failure or deployment error.\nMonitoring \u0026amp; Alerting: Set up CloudWatch alarms to detect performance, security, and cost issues early, send email notifications to the operations team.\nContinuous Improvement: Periodically evaluate CI/CD processes, automate testing, optimize pipelines, and update documentation to ensure the system is always stable and easy to maintain.\n8. Expected Results Technical Results:\nComplete a serverless student management system with an automated CI/CD process, ensuring fast and stable build, test, and deployment.\nAPI fully supports student management functions, integrates Lambda workflows, integration testing with Postman/SWAGGER.\nModern frontend with React/TypeScript, 15+ pages, 50+ UI components, realtime connection with backend.\nIntegrates key AWS services: API Gateway, Lambda, DynamoDB, Cognito, S3, Amplify, CloudWatch, Route53, CloudFront, WAF, CodePipeline, CodeBuild, CodeDeploy.\nPerformance: API response \u0026lt;500ms, uptime ≥99%, CI/CD process optimizes cost and deployment time.\nActual cost: $7-20/month, total ~$21-60 for 3 months (can be reduced to $15-30 with AWS Free Tier and Educate credits).\nLong-term value:\nEasily expandable to mobile apps or advanced AI/analytics integration. A practice and training platform for serverless, CI/CD, and DevOps for students and small businesses. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand Amazon VPC architecture and its basic components. Master network design on AWS. Practice creating VPC, Subnets, Route Tables, Internet Gateway, and NAT Gateway. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Study Theory Module 02-01: Amazon VPC + VPC concept + Subnet (Public/Private) + Route table + Elastic Network Interface (ENI) + Elastic IP address (EIP) + Internet Gateway + NAT Gateway 15/09/2025 15/09/2025 https://www.youtube.com/@AWSStudyGroup 3 - Study Theory Module 02-02: VPC Security + Security Group + Webserver \u0026amp; Database security group + Network Access Control List (NACL) + VPC Flow Logs + VPC Peering \u0026amp; Transit Gateway 16/09/2025 16/09/2025 https://www.youtube.com/@AWSStudyGroup 4 - Study Theory Module 02-03: VPN - DirectConnect - LoadBalancer + VPN \u0026amp; Direct Connect + Elastic Load Balancing (ALB, NLB, CLB, GLB) 17/09/2025 17/09/2025 https://www.youtube.com/@AWSStudyGroup 5 - Practice Lab03-01: + Introduction to Amazon VPC and AWS VPN Site-to-Site + Create Subnets + Create Route table + Create Internet Gateway (IGW) + Create NAT Gateway 18/09/2025 18/09/2025 https://www.youtube.com/@AWSStudyGroup 6 - Practice Lab03-02: + Create Security Group + Configure Network ACLs + Explore VPC Resource Map 19/09/2025 19/09/2025 https://www.youtube.com/@AWSStudyGroup Week 2 Achievements: Understood Amazon VPC architecture and its components:\nVPC, Subnets (Public/Private) Route Tables and Internet Gateway NAT Gateway for private subnets Elastic IP and Elastic Network Interface Mastered network security concepts:\nSecurity Groups (stateful firewall) Network ACLs (stateless firewall) VPC Flow Logs for traffic monitoring Understood connectivity methods:\nVPC Peering and Transit Gateway VPN and Direct Connect Types of Load Balancer (ALB, NLB, CLB, GLB) Successfully practiced:\nCreating VPC with public and private subnets Configuring Route Tables Setting up Internet Gateway and NAT Gateway Creating Security Groups and Network ACLs "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2 - Transfer data from Amazon S3 to IoT Edge device",
	"tags": [],
	"description": "",
	"content": "Transferring Data from Amazon S3 to IoT Edge Device Written by Rashmi Varshney, Nilo Bustani, and Tamil Jayakumar | June 28, 2025 | in AWS IoT Core, AWS IoT Greengrass, Learning Levels, Technical How-to | Permalink | Share\nSeamless data transfer between the cloud and edge devices is critical for IoT applications in many industries such as healthcare, manufacturing, autonomous vehicles, and aerospace. For example, it enables aircraft operators to seamlessly transfer software updates to the entire fleet without manual physical storage devices. By leveraging AWS IoT and Amazon Simple Storage Service (Amazon S3), you can set up a data transfer mechanism that enables real-time and historical data exchange between the cloud and edge devices.\nIntroduction This article guides you step-by-step on how to transfer data as files from Amazon S3 to your IoT Edge device.\nWe will use AWS IoT Greengrass, an open-source edge runtime and cloud service to build, remotely deploy, and manage device software on millions of devices. IoT Greengrass provides built-in components for common use cases, allowing you to discover, import, configure, and deploy applications and services at the edge without needing to understand different device protocols, manage credentials, or interact with external APIs. You can also create custom components based on your IoT use case.\nIn this article, we will build and deploy a custom IoT Greengrass component leveraging the capabilities of Amazon S3 Transfer Manager. This IoT Greengrass component performs actions such as downloading via IoT Jobs topics. The parameters set in IoT Jobs define these actions.\nS3 Transfer Manager uses multipart upload APIs and byte-range fetches to transfer files from Amazon S3 to edge devices. You can read more about S3 Transfer Manager capabilities in the original blog.\nPrerequisites To simulate an edge device, we will use an EC2 instance. Before proceeding with the steps to transfer files from Amazon S3 to your instance, ensure you have the following:\nAn AWS account with permissions to create and access Amazon EC2 instances, AWS Systems Manager (SSM), AWS CloudFormation stacks, AWS IAM Roles and Policies, Amazon S3, AWS IoT Core. AWS CLI installed and configured on your machine, along with the SSM Manager Plugin. Follow the steps in the Visual Studio Code on EC2 for Prototyping repository to deploy an EC2 instance. Use the browser-based VS Code IDE to edit files and execute instructions. When deploying, the EC2 instance will be attached to an IAM Role that allows unrestricted access to all AWS resources. We recommend reviewing the role attached to the EC2 instance and editing it to restrict permissions to only SSM, S3, IoT Core, and IoT Greengrass.\nSolution Overview Transferring files from Amazon S3 to an edge device involves creating a custom IoT Greengrass component called “Download Manager.” This component is responsible for downloading files from Amazon S3 to the edge device, in this case, an EC2 instance simulating an edge device. The process can be broken down into the following steps:\nStep 1: Develop and package the custom IoT Greengrass Download Manager Component, which handles file transfer logic. After packaging, upload this component to the designated Component and Content Bucket on Amazon S3. Step 3: Upload the files to be transferred to the ‘Component and Content Bucket’ on Amazon S3. Step 4: The Download Manager Component on the edge device (EC2) will download files from the Amazon S3 bucket and save them to the file system on the edge device. Figure 1 – Transferring files from Amazon S3 to an EC2 instance simulating an edge device\nStep 1: Develop and Package the Custom IoT Greengrass Download Manager Component 1.1 Clone the custom IoT Greengrass component from the aws-samples repository\ngit clone https://github.com/aws-samples/sample-asset-transfer-manager-for-edge-iot.git cd download-manager 1.2 Follow the instructions to configure the EC2 instance as an IoT Greengrass core device.\n1.3 The IoT Greengrass Development Kit Command-Line Interface (GDK CLI) reads from the gdk-config.json configuration file to build and publish the component. Update the gdk-config.json file, replace us-west-2 with the region you are deploying to, and update gdk_version to match your GDK CLI version:\n{ \u0026#34;component\u0026#34;: { \u0026#34;com.example.DownloadManager\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Amazon\u0026#34;, \u0026#34;build\u0026#34;: { \u0026#34;build_system\u0026#34;: \u0026#34;zip\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;zip_name\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;publish\u0026#34;: { \u0026#34;bucket\u0026#34;: \u0026#34;greengrass-artifacts\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-2\u0026#34; } } }, \u0026#34;gdk_version\u0026#34;: \u0026#34;1.3.0\u0026#34; } Step 2: Build, publish and deploy the Download Manager component 2.1 You can build and publish Download Manager Component to Amazon S3 bucket according to instructions here.\nThis step will automatically create an Amazon S3 bucket named greengrass-artifacts-YOUR_REGION-YOUR_AWS_ACCOUNT_ID. The components built are stored as objects in this Amazon S3 bucket. We will use this Amazon S3 bucket to publish the custom Download Manager component and also use it to store the assets that will be downloaded to the EC2 instance.\n2.2 Follow the instructions mentioned here to allow the Greengrass core IoT device to access the Amazon S3 bucket.\n2.3 After successfully publishing the Download Manager component, you can find it in AWS Management Console → AWS IoT Core → Greengrass Devices → Components → My Components.\nFigure 2 – AWS IoTCore Greengrass Components List\n2.4 To enable file transfers from an Amazon S3 bucket to an edge device, we will deploy the Download Manager component to a simulated Greengrass device running on an EC2 instance. From the components list above, click on the component titled com.example.DownloadManager and click Deploy, select Create new deployment and click Next.\n2.5 Enter the deployment name as \u0026ldquo;My Deployment\u0026rdquo; and the \u0026ldquo;Deployment Target\u0026rdquo; as \u0026ldquo;Core Device\u0026rdquo;. Enter the core device name found in AWS Management Console → AWS IoT Core → Greengrass Devices → Core devices, then click \u0026ldquo;Next\u0026rdquo;.\n2.6 Select Components: Along with the custom component, we will also deploy the public components provided by AWS listed below:\naws.greengrass.Nucleus – The IoT Greengrass kernel component is a mandatory component and is the minimum requirement to run the IoT Greengrass Core software on the edge.\naws.greengrass.Cli – The IoT Greengrass CLI component provides a local command line interface that you can use on the edge to develop and debug local components. The IoT Greengrass CLI allows you to create local deployments and reboot components on the edge.\naws.greengrass.TokenExchangeService – The token exchange service provides AWS credentials that can be used to interact with AWS services from the custom components. This is required for the boto3 library to download files from the Amazon S3 bucket to the edge.\nFigure 3 – Selecting Components to Deploy\n2.7 Component Configuration: From the list of Public components, configure the Nucleus component and set the `interpolateComponentConfiguration` flag to true. This option should be set to true so that the edge device can run Greengrass IoT components using the recipe variables from the configuration. This will also reference the thingName in the codebase from the AWS_IOT_THING_NAME environment variable and there is no need to hardcode the thingName.\nIn the Component Configuration list, select the Nucleus component and click Configure Component. Update the Configuration section to Merge as follows and click Confirm.\n{ \u0026#34;interpolateComponentConfiguration\u0026#34;: true } Figure 4 – Configuring aws.greengrass.Nucleus\n2.8 Keep the default deployment configuration and proceed to the Review page and click Deploy.\n2.9 You can monitor the process by viewing the IoT Greengrass log file on the simulated IoT Greengrass device running on the EC2 instance. You will see \u0026ldquo;status=SUCCEEDED\u0026rdquo; in the log.\nsudo tail -f /greengrass/v2/logs/greengrass.log\n2.10 After successful deployment, you can monitor the logs for the custom Download Manager component on the simulated Greengrass IoT device running on the EC2 instance as shown below. You will see currentState=RUNNING in the log.\nsudo tail -f /greengrass/v2/logs/com.example.DownloadManager.log\n2.11 The download directory is configured to /opt/downloads when deploying the Download Manager component Custom Download. Monitoring the download process\nsudo su cd /opt/downloads ls Step 3: Upload the file to the edge device The Download Manager component facilitates the transfer of files from Amazon S3 to your edge device. AWS IoT Jobs plays an important role in this process by allowing you to define and execute remote operations on connected devices. With AWS IoT Jobs, you can create a job that instructs the edge device to download a file from a specified Amazon S3 bucket location. This job acts as a set of instructions, instructing the Download Manager component to search for the desired file in the Amazon S3 bucket. Once the job is created and sent to the edge device, the Download Manager component initiates the download process, seamlessly transferring the specified files from Amazon S3 to the edge device\u0026rsquo;s local storage.\n3.1 Create a folder named uploads in the Amazon S3 bucket (greengrass-artifacts-YOUR_REGION-YOUR_AWS_ACCOUNT_ID) created in Step 2.1. Upload the image generated by GenAI below named owl.png to the uploads folder in the Amazon S3 bucket.\nFigure 5 – Image generated by GenAI – owl.png\nFor simplicity, we are reusing the same Amazon S3 bucket(greengrass-artifacts-YOUR_REGION-YOUR_AWS_ACCOUNT_ID). However, it is best to create 2 separate buckets for the Greengrass IoT components and the files that need to be downloaded from the edge.\n3.2 Once the file has been uploaded to the Amazon S3 bucket, copy the S3 URI of this image for use in the next step. The S3 URI will be s3://greengrass-artifacts-REGION-ACCOUNT_ID/uploads/owl_logo.png.\nStep 4: Write a Python script to sync data 4.1 Create an AWS IoT Job Document\n4.1.1 From the AWS Management Console, navigate to AWS IoT Core → Remote actions→ Jobs and click Create job.\n4.1.2 Choose to create a custom job\n4.1.3 Name the job, e.g., Test-1, and optionally provide a description, then click Next.\n4.1.4 For Job Target, select the core device specified by device name\u0026lt; YOUR GREENGRASS DEVICE NAME \u0026gt;. You can leave the Thing group blank for now.\n4.1.5 Select Job document From from the template and select AWS-Download-File from the Template.\n4.1.6 Paste the S3 URI into the downloadUrl section. The S3 URI must start with s3://greengrass-artifacts-REGION-ACCOUNT_ID/uploads/owl_logo.png\n4.1.7 For File Path, enter the subdirectory where you want the file to be downloaded. For this blog, we will create a folder called images and click Next. Do not add a / to the path as the component will automatically prefix the path.\n4.1.8 To configure the task and run type, select Snapshot and click Submit.\n4.2 Monitor the component log on the EC2 instance to see the download folder being created and the image named owl.png being downloaded.\nsudo tail -f /greengrass/v2/logs/com.example.DownloadManager.log\n4.3 Monitor the Task Progress: Each Task document also supports updating the execution status from the task level and the thing level. From AWS Management Console → Jobs → Test-1→ Job executions.\nFigure 6 – Tracking Job Execution\n4.4 To view the execution status from the edge device, click the checkbox for the core device in the Job Execution section.\nFigure 7 – Viewing Job Execution Status Details 4.5 Once the file has been downloaded to the EC2 instance, you can find it in the /opt/downloads/images folder in the core device.\nsudo su # cd /opt/downloads/images/ # ls -alh total 1.1M drwxrwxr-x 2 ggc_user ggc_group 4.0K Jun 13 17:10 . drwx------- 3 ggc_user root 4.0K Jun 13 17:10 .. -rw-rw-r-- 1 ggc_user ggc_group 1.1M Jun 13 17:10 owl_logo.png Cleanup To ensure cost efficiency, this blog uses the AWS Free Tier for all services, except for the EC2 instance and the EBS volumes attached to it. The EC2 instance used in this example requires an On-Demand t3.medium instance on demand to house both the development environment and the simulated edge device in the same underlying EC2 instance. For more information, please refer to the pricing details. After completing this tutorial, be sure to go to the AWS Console and delete the resources created during this process by following the instructions provided. This step is important to avoid any unexpected charges in the future.\nCleanup Instructions:\nOpen S3 from the AWS console and delete the contents of the Amazon S3 bucket named greengrass-artifacts-YOUR_REGION-YOUR_AWS_ACCOUNT_ID and the Amazon S3 bucket.\nOpen IoT Core from the AWS console and delete all tasks from the IoT Jobs Manager Dashboard.\nOpen IoT Greengrass from the AWS console and delete the IoT thing Group, Object, Certificate, Policy, and Role associated with MyGreengrassCore.\nFollow the cleanup instructions in the aws-samples VS Code on EC2 repository.\nCustomer References AWS customers are using this approach to transfer files from Amazon S3 to edge devices.\nConclusion This blog post illustrates how AWS customers can efficiently move data from Amazon S3 to their edge devices. The detailed steps enable seamless downloads of software updates, firmware updates, content, and other essential files. Real-time monitoring provides complete visibility and control of all file transfers. You can further optimize your operations by implementing the pause and resume functionality mentioned in the blog. Additionally, you can use AWS IoT Greengrass and Amazon S3 Transfer Manager to implement upstream data flows from edge devices to Amazon S3. Furthermore, through a custom IoT Greengrass component, you can facilitate the upload of logs and telemetry data, opening up powerful opportunities for predictive maintenance, real-time analytics, and data-driven insights.\nAbout the Authors Tamil Jayakumar Tamil Jayakumar is a Dedicated Solutions Architect \u0026amp; Prototype Engineer at Amazon Web Services. He has over 14 years of experience in software development, developing Proof of Concept, creating Minimum Viable Products (MVPs) using end-to-end application development and solution architect skills. He is a hands-on technologist, passionate about solving technology challenges with innovative software and hardware solutions, combining business needs with IT capabilities.\nRashmi Varshney Rashmi Varshney is a Senior Solutions Architect at Amazon Web Services, based in Austin. She has over 20 years of experience, primarily in analytics. She is passionate about helping customers build cloud adoption strategies, design innovative solutions, and drive operational excellence. As a member of the AWS Analytics Engineering Community, she actively contributes to industry collaboration efforts. Nilo Bustani Nilo Bustani is a Senior Solutions Architect at AWS with over 20 years of experience in application development, cloud architecture, and technical leadership. She specializes in helping customers build strong observability strategies and governance practices across hybrid and multi-cloud environments. She is dedicated to providing organizations with the tools and practices needed to succeed in their journey to cloud and AI transformation.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2 - DevOps on AWS\u0026rdquo; Event Objectives Share DevOps culture and principles on AWS Guide on building complete CI/CD pipelines Introduce Infrastructure as Code with CloudFormation and CDK Practice deploying container services and monitoring Apply DevOps best practices to enterprise environments Key Highlights Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nDevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) Importance of collaboration between Dev and Ops 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, drift detection AWS CDK: Constructs, reusable patterns, language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Afternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment strategies: Feature flags, A/B testing\nAutomated testing and CI/CD integration\nIncident management and postmortems\nCase Studies: Startups and enterprise DevOps transformations\n3 integration patterns: Publish/Subscribe, Point-to-point, Streaming\nBenefits: Loose coupling, scalability, resilience\nSync vs async comparison: Understanding the trade-offs\nCompute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.4-frontend/5.4.3-route53/",
	"title": "Amazon Route53",
	"tags": [],
	"description": "",
	"content": "ROUTE53 (Custom Domain) 1. Create Hosted Zone Go to AWS Console → Route53 → Create hosted zone Domain name: yourdomain.com Type: Public hosted zone 2. Configure DNS Records A Record for CloudFront:\nRecord name: (empty or www)\rRecord type: A\rAlias: Yes\rRoute traffic to: CloudFront distribution CNAME for Amplify (if not using CloudFront):\nRecord name: app\rRecord type: CNAME\rValue: xxx.amplifyapp.com 3. SSL Certificate (ACM) Go to AWS Console → Certificate Manager Request certificate (must be in region us-east-1 for CloudFront) Domain: yourdomain.com, *.yourdomain.com Validation: DNS validation Add CNAME records to Route53 Summary Page \u0026ldquo;Building a Frontend: Amplify, ROUTE53, CloudFront, WAF\u0026rdquo; full guide:\nCreate a React project with Amplify Use Cognito for frontend authentication Hosting CI/CD via Amplify Frontend connects directly to the serverless backend system, creating a smooth and secure real-time experience.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.3-backend/5.3.3-amazons3/",
	"title": "Amazon S3",
	"tags": [],
	"description": "",
	"content": "Amazon S3 S3 is used for:\nSave student avatar\nClassroom materials\nBuild artifacts (React/Vite)\nDeploy website via S3 + CloudFront\nLog \u0026amp; learning material assets\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.3-backend/",
	"title": "Backend Deployment",
	"tags": [],
	"description": "",
	"content": "Building a Serverless Backend with AWS Lambda, API Gateway, DynamoDB, and Cognito Overview This section guides you through the backend implementation of a serverless student management system on AWS. You will use core services such as DynamoDB for data storage, Lambda for business logic, API Gateway to connect the frontend and backend, and Cognito for user authentication. The implementation process includes designing the data table, configuring authentication, building the backend application with Java Spring Boot, packaging and deploying to Lambda, as well as configuring API Gateway to serve student management tasks in a secure, automated, and scalable way.\nContent Amazon DynamoDB Amazon Cognito Amazon S3 LAMBDA + API GATEWAY "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Simulating Partial Failures with AWS Fault Injection Service This blog guides you through simulating partial failures in distributed systems using AWS Fault Injection Service (FIS). You will understand why resilience testing is needed by controlled fault injection, how to combine FIS with Application Load Balancer and an AWS Lambda function to redirect a portion of traffic to simulated error responses, and the benefits of this approach for evaluating fault tolerance without disrupting the entire service. The article also presents practical deployment steps—from deploying a CloudFormation template, verifying the Lambda error response, launching the FIS experiment, observing application behavior and automatic rollback—along with notes on IAM permissions, testing in non-production environments, and cleaning up resources after completion.\nBlog 2 - Transferring Data from Amazon S3 to IoT Edge Devices This blog provides a step-by-step guide to transferring files from Amazon S3 to IoT Edge devices using AWS IoT Greengrass and Amazon S3 Transfer Manager; you will learn how to build and package a custom component (Download Manager) to download content, deploy that component to a simulated device on EC2, and use AWS IoT Jobs to orchestrate download commands. The article also lists prerequisites (AWS account, EC2, IAM, S3, CLI), configuration and deployment instructions, how to monitor logs and job status to verify downloads, and resource cleanup steps after completion—while highlighting practical benefits such as software updates, firmware delivery, and telemetry collection from edge devices.\nBlog 3 - Connecting Amazon WorkSpaces Personal with AWS PrivateLink This blog provides a detailed guide on integrating AWS PrivateLink with Amazon WorkSpaces Personal to establish a private, secure streaming connection without using the public internet; you will understand how PrivateLink reduces the attack surface and keeps all traffic within the AWS network. The article presents practical benefits, deployment steps—from creating the appropriate security group, setting up a VPC Interface Endpoint with DNS and subnet configuration, to configuring the WorkSpaces directory to use the endpoint—along with notes on IP record type (IPv4 only), streaming compatibility, IAM permissions, and recommendations to test only in non-production environments. Finally, the blog outlines verification steps (confirming endpoint status, testing new streaming sessions), options to allow internet streaming for some WorkSpaces, and resource cleanup after completion.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Practice deploying a complete VPC with EC2 instances. Understand how to connect EC2s in different subnets. Practice EC2 Instance Connect Endpoint. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Practice Lab03-03: Create VPC from scratch + Create VPC + Create Subnets (Public/Private) + Create Internet Gateway + Create Route Table for Outbound Internet Routing 22/09/2025 22/09/2025 https://www.youtube.com/@AWSStudyGroup 3 - Practice Lab03-03 (cont.): + Create Security Groups + Configure Inbound/Outbound rules 23/09/2025 23/09/2025 https://www.youtube.com/@AWSStudyGroup 4 - Practice Lab03-04: EC2 Instances in Subnets + Create EC2 Instances in Public Subnet + Create EC2 Instances in Private Subnet + Test connectivity between instances 24/09/2025 24/09/2025 https://www.youtube.com/@AWSStudyGroup 5 - Practice Lab03-04 (cont.): + Create NAT Gateway + Configure Route Table for Private Subnet + Test Internet connectivity from Private Subnet 25/09/2025 25/09/2025 https://www.youtube.com/@AWSStudyGroup 6 - Practice: EC2 Instance Connect Endpoint + Create EC2 Instance Connect Endpoint + Connect to Private EC2 without Bastion Host + Clean up resources 26/09/2025 26/09/2025 https://www.youtube.com/@AWSStudyGroup Week 3 Achievements: Successfully practiced creating a complete VPC:\nCreated VPC with appropriate CIDR block Created Public and Private Subnets Configured Internet Gateway and NAT Gateway Set up Route Tables for each subnet type Deployed EC2 instances in VPC:\nCreated EC2 in Public Subnet with Public IP Created EC2 in Private Subnet Tested SSH and Internet connectivity Used EC2 Instance Connect Endpoint:\nUnderstood how to connect to Private EC2 without Bastion Host Saved costs and increased security Learned how to clean up resources to avoid unnecessary costs:\nTerminate EC2 instances Delete NAT Gateway Release Elastic IPs Delete VPC and related components "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Desktop and Application Streaming",
	"tags": [],
	"description": "",
	"content": "Connecting Amazon WorkSpaces Personal with AWS PrivateLink Written by Dave Jaskie, Gekai Zou, Aamir Khan, and Anshu Prabhat | June 26, 2025 | in Amazon WorkSpaces, AWS PrivateLink, End User Computing | Permalink | Share\nCustomers often ask how to leverage AWS PrivateLink to connect with Amazon WorkSpaces Personal. PrivateLink provides a seamless way to establish private, secure connectivity without using traditional network components like internet gateways, NAT devices, or VPN configuration. This approach not only simplifies network architecture but also enhances security by significantly reducing the attack surface and keeping all data traffic safely within the AWS network. In this article, we’ll walk you through the steps to integrate PrivateLink with WorkSpaces, helping you unlock these benefits in your environment.\nPrerequisites and Limitations PrivateLink for WorkSpaces is currently supported for streaming traffic. See the Admin Guide for details on prerequisites and limitations with WorkSpaces. To configure PrivateLink, you first need to set up a security group, create a VPC endpoint, and finally configure the WorkSpaces directory to use the VPC endpoint.\nStep 1: Create a Security Group In this step, you create a security group that allows WorkSpaces clients to communicate with the VPC endpoint you’ll create.\nIn the navigation pane of the Amazon EC2 console, go to Network \u0026amp; Security, then select Security Groups. Select Create security group. In Basic details, enter:\nFor Security group name – Enter a unique name to identify the security group.\nFor Description – Enter a description of the security group’s purpose.\nFor VPC – Select the VPC where your VPC endpoint will reside. In Inbound rules, select Add rule to create an inbound rule for TCP. Enter the following details:\nFor Type – Select “Custom TCP”.\nFor Port range – Enter ports: 443, 4195.\nFor Source type – Select “Custom”.\nFor Source – Enter the private IP CIDR range or other Security Group IDs that your users connect to the VPC endpoint from. Ensure only traffic from IPv4 addresses is allowed. Make sure to allow inbound traffic from IPv4 addresses. Repeat steps 4 and 5 for each CIDR range or security group. In Inbound rules, select Add rule to create an inbound rule for UDP. Enter:\nFor Type – Select “Custom TCP”.\nFor Port range – Enter ports: 443, 4195.\nFor Source type – Select “Custom”.\nFor Source – Enter the same private IP CIDR range or Security Group IDs entered in Step 5. Repeat steps 7 and 8 for each custom UDP source. Select Create security group. Step 2: Create a VPC Endpoint In Amazon VPC, a VPC endpoint allows you to connect your VPC to supported AWS services. In this example, you configure the VPC so WorkSpaces users can stream from WorkSpaces.\nOpen the Amazon VPC console. In the navigation pane, go to Endpoints, then select Create Endpoint. Select Create Endpoint. Ensure the following settings:\nService category – Select “AWS services”.\nService Name – Select com.amazonaws.Region.prod.highlander.\nVPC – Select the VPC where you want to create the interface endpoint. You can select a VPC different from the one hosting WorkSpaces resources as long as the network can route traffic to the VPC endpoint.\nEnable Private DNS Name – Deselect if users use a network proxy to access streaming sessions; disable proxy caching on the domain and DNS names related to the private endpoint.\nDNS record IP type – Select IPv4, as IPv6 and Dualstack are not currently supported.\nSubnets – Select the subnets (Availability Zones) to create the VPC endpoint. It’s recommended to select at least two subnets.\nIP address type – Select IPv4.\nSecurity groups panel – Select the security group you created in step 1.\n(Optional) In the Tags tab, you can add tags if desired. Select Create endpoint. When the endpoint is ready, the Status column will show Available. Step 3: Configure the WorkSpaces Directory to Use the VPC Endpoint You need to configure the WorkSpaces directory to use the VPC endpoint you created for streaming.\nOpen the WorkSpaces console in the same AWS region as the VPC endpoint. In the Navigation pane, select Directories. Select the directory you want to use. In the VPC Endpoints section, select Edit. In the Edit VPC Endpoint dialog box, under Streaming Endpoint, select the VPC endpoint you created. Optional: You can enable Allow users with PCoIP WorkSpaces to stream from the internet. If enabled, PCoIP WorkSpaces users can stream over the public Internet. If not enabled, PCoIP WorkSpaces in the directory will not work without internet because PCoIP WorkSpaces do not support streaming via VPC endpoint. Select Save.\nNote: Traffic for new streaming sessions will be routed through this VPC endpoint. However, running sessions will continue to use the previously configured endpoint. Conclusion In this article, we discussed how to set up Amazon WorkSpaces Personal with AWS PrivateLink. For more information about PrivateLink, see the AWS product page. If you have questions, you can contact the AWS support team. To stay updated on new End User Compute features, check “What’s New with AWS” and the corresponding YouTube playlist.\nAuthors Dave Jaskie is a Senior End User Computing Solutions Architect at AWS, with 15 years of experience in end user computing. Outside of work, Dave enjoys traveling and hiking with his wife and four children.\nAamir Khan is a Senior Principal Technical Program Manager in the End User Computing Product team, with 12 years of industry experience. With a customer-first approach, his work sets a standard. Outside of his professional role, Aamir enjoys family moments and occasionally off-roading in his LC-100, exploring the wonders of the Pacific Northwest.\nAnshu Prabhat is a Software Development Engineer III at AWS in the Safe Work Support Organization with over 12 years of experience in secure cloud computing and scalable services at Amazon. Anshu enjoys exploring the world of GenAI and strives to automate routine personal tasks using Agents.\nGekai Zou is a Senior Principal Product Engineer for AWS End User Computing. Gekai has been with AWS since 2019. Outside of work, Gekai enjoys camping and skiing with his family.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3 - Security on AWS (Well-Architected Security Pillar)\u0026rdquo; Event Objectives Introduce AWS Well-Architected Security Pillar and 5 security pillars Share best practices for Identity \u0026amp; Access Management (IAM) Guide on implementing Detection and Continuous Monitoring Practice Infrastructure Protection and Data Protection Build Incident Response playbooks and automation Key Highlights 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Role of Security Pillar in Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth Shared Responsibility Model: Defining responsibilities between AWS and customers Top security threats in cloud environment in Vietnam Pillar 1 — Identity \u0026amp; Access Management (8:50 – 9:30 AM) Modern IAM Architecture\nIAM fundamentals: Users, Roles, Policies – avoid long-term credentials IAM Identity Center: SSO implementation, permission sets SCP \u0026amp; permission boundaries: Multi-account governance MFA, credential rotation, Access Analyzer: Security best practices Mini Demo: Validate IAM Policy + simulate access scenarios Pillar 2 — Detection (9:30 – 9:55 AM) Detection \u0026amp; Continuous Monitoring\nCloudTrail: Organization-level audit logging GuardDuty: Threat detection and intelligent monitoring Security Hub: Centralized security posture management Logging strategy: VPC Flow Logs, ALB logs, S3 access logs Alerting \u0026amp; automation: EventBridge integration Detection-as-Code: Infrastructure and security rules as code Pillar 3 — Infrastructure Protection (10:10 – 10:40 AM) Network \u0026amp; Workload Security\nVPC segmentation: Public vs private subnet placement strategies Security Groups vs NACLs: Layered security model WAF + Shield + Network Firewall: Protection against attacks Workload protection: EC2, ECS/EKS security fundamentals Pillar 4 — Data Protection (10:40 – 11:10 AM) Encryption, Keys \u0026amp; Secrets Management\nAWS KMS: Key policies, grants, automatic rotation Encryption strategies: At-rest: S3, EBS, RDS, DynamoDB In-transit: TLS/SSL, VPN, encryption in application layer Secrets Manager \u0026amp; Parameter Store: Secrets rotation patterns Data classification \u0026amp; access guardrails: Compliance requirements Pillar 5 — Incident Response (11:10 – 11:40 AM) IR Playbook \u0026amp; Automation\nIncident Response lifecycle: Preparation, Detection, Analysis, Containment, Eradication, Recovery Common IR playbooks: Compromised IAM credentials S3 bucket public exposure EC2 malware detection Response automation: Lambda/Step Functions for automated remediation Evidence collection: Snapshot, isolation, forensics Key Takeaways Security Foundation Well-Architected Security Pillar: Comprehensive framework for cloud security Security principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility Model: Understanding AWS and customer responsibilities Threat landscape: Top security threats in cloud environment Identity \u0026amp; Access Management IAM best practices: Avoid long-term credentials, use roles IAM Identity Center: Centralized SSO and permission management Multi-account governance: SCPs and permission boundaries Access validation: IAM Access Analyzer and policy simulation Detection \u0026amp; Monitoring Comprehensive logging: CloudTrail, VPC Flow Logs, application logs Threat detection: GuardDuty intelligent monitoring Centralized security: Security Hub for posture management Automated response: EventBridge integration with Lambda Infrastructure Protection Network segmentation: Proper VPC architecture design Layered security: Security Groups + NACLs + WAF/Shield Attack protection: DDoS protection, web application firewall Workload security: Container and compute security Data Protection Encryption everywhere: At-rest and in-transit encryption Key management: KMS best practices, rotation strategies Secrets management: Automated rotation with Secrets Manager Data classification: Compliance and access control Incident Response IR preparedness: Playbooks and response procedures Automation: Auto-remediation with serverless Forensics: Evidence collection and analysis Continuous improvement: Post-incident reviews Applying to Work Implement IAM best practices: Review and refactor IAM policies Enable comprehensive logging: CloudTrail, GuardDuty, Security Hub Apply encryption: Enable encryption for all data stores Develop IR playbooks: Create response procedures for common incidents Automate security: Build automated security responses Security training: Share knowledge with team about security best practices Event Experience Attending the \u0026ldquo;AWS Cloud Mastery Series #3 - Security on AWS\u0026rdquo; workshop was a focused and in-depth morning on cloud security, helping me better understand how to build a secure system on AWS. Key experiences included:\nLearning from Security experts Speakers from AWS shared the Well-Architected Security Pillar with 5 important security pillars. Through demos and case studies, I gained a better understanding of the threat landscape and common security threats. Comprehensive security practice Participating in IAM sessions helped me understand modern identity management and Zero Trust approach. Learned how to implement detection and monitoring with CloudTrail, GuardDuty, Security Hub. Practiced infrastructure protection with VPC segmentation, WAF, Shield. Data Protection and Encryption Deep dive into AWS KMS and encryption strategies for every layer. Learned how to manage secrets and implement automatic rotation. Understood data classification and compliance requirements. Incident Response Workshop provided practical IR playbooks for common scenarios. Learned how to automate response with Lambda and Step Functions. Understood forensics process and evidence collection. Networking and discussions Exchanged ideas with security professionals about real-world security challenges. Discussed security threats specific to Vietnam. Networked with security engineers and architects. Lessons learned Security is everyone\u0026rsquo;s responsibility: Not just the security team\u0026rsquo;s job. Defense in depth: Multiple layers of security controls. Automation is critical: Automate detection and response. Continuous monitoring: Security is a continuous process, not a one-time setup. Preparedness matters: IR playbooks and automation help respond faster. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.4-frontend/",
	"title": "Frontend Development",
	"tags": [],
	"description": "",
	"content": "Building a Secure Frontend with AWS Amplify, CloudFront, WAF, and Route53 Overview In this section, you will build a web interface for the Serverless Student Management System using AWS Amplify, CLOUDFRONT, WAF, and ROUTE53.\nFrontend provides the following functions:\nOverall architecture:\nContent Amazon Amplify Amazon CloudFront \u0026amp; WAF Amazon Route53 "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.3-backend/5.3.4-lambdaapigateway/",
	"title": "LAMBDA + API GATEWAY",
	"tags": [],
	"description": "",
	"content": "LAMBDA + GATEWAY API (Backend) 1. Prepare Java Spring for Lambda Add dependencies to pom.xml:\n\u0026lt;dependencies\u0026gt; \u0026lt;!-- AWS Lambda --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.amazonaws.serverless\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aws-serverless-java-container-springboot3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- AWS SDK --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.awssdk\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamodb\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.21.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.0\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt;\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt;\u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; Create Lambda Handler:\n// src/main/java/com/example/StreamLambdaHandler.java package com.example; import com.amazonaws.serverless.exceptions.ContainerInitializationException; import com.amazonaws.serverless.proxy.model.AwsProxyRequest; import com.amazonaws.serverless.proxy.model.AwsProxyResponse; import com.amazonaws.serverless.proxy.spring.SpringBootLambdaContainerHandler; import com.amazonaws.services.lambda.runtime.Context; import com.amazonaws.services.lambda.runtime.RequestStreamHandler; import java.io.IOException; import java.io.InputStream; import java.io.OutputStream; public class StreamLambdaHandler implements RequestStreamHandler { private static SpringBootLambdaContainerHandler\u0026lt;AwsProxyRequest, AwsProxyResponse\u0026gt; handler; static { try { handler = SpringBootLambdaContainerHandler.getAwsProxyHandler(Application.class); } catch (ContainerInitializationException e) { throw new RuntimeException(\u0026#34;Could not initialize Spring Boot application\u0026#34;, e); } } @Override public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context) throws IOException { handler.proxyStream(inputStream, outputStream, context); } } 2. Build JAR cd backend-project mvn clean package -DskipTests # Output: target/your-app.jar 3. Create Lambda Function Go to AWS Console → Lambda → Create function Configuration: Function name: student-management-api Runtime: Java 17 Architecture: x86_64 Memory: 512 MB (or 1024 MB for better performance) Timeout: 30 seconds Upload JAR file or from S3 Handler: com.example.StreamLambdaHandler::handleRequest 4. Configure IAM Role for Lambda Attach policies:\nAmazonDynamoDBFullAccess AWSLambdaBasicExecutionRole AmazonCognitoPowerUser 5. Create API Gateway Go to AWS Console → API Gateway → Create API Select HTTP API (recommended) or REST API Configuration: API name: student-management-api Integration: Lambda function Route: ANY /{proxy+} Enable CORS: Access-Control-Allow-Origin: * (or specific domain) Access-Control-Allow-Methods: GET, POST, PUT, PATCH, DELETE, OPTIONS Access-Control-Allow-Headers: Content-Type, Authorization Deploy API: Create stage: prod Save Invoke URL: https://xxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod Summary This section guides you through the implementation of a backend for a serverless student management system on AWS, using key services such as DynamoDB, Lambda, API Gateway, and Cognito. You have practiced:\nDesigning and creating DynamoDB tables to store student, class, subject, and notification data, and configuring GSI to optimize queries. Setting up Cognito User Pool to authenticate and authorize users. Using S3 to store avatars, class materials, and build artifacts. Building a backend with Java Spring Boot, packaging the application into a JAR, and deploying it to Lambda. Configuring IAM Roles for Lambda to ensure necessary service access. Create and configure API Gateway to connect frontend with backend, fully support HTTP methods and CORS security. This process helps you build a modern, automated, scalable and secure backend, fully meeting student management tasks on AWS platform.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Set up Hybrid DNS with Route 53 Resolver. Understand and practice VPC Peering to connect multiple VPCs. Use CloudFormation to deploy infrastructure. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Practice Lab10: Hybrid DNS with Route 53 Resolver + Introduction to Hybrid DNS + Generate Key Pair + Initialize CloudFormation Template 29/09/2025 29/09/2025 https://www.youtube.com/@AWSStudyGroup 3 - Practice Lab10 (cont.): + Configure Security Group + Connect to RDGW (Remote Desktop Gateway) + Set up DNS 30/09/2025 30/09/2025 https://www.youtube.com/@AWSStudyGroup 4 - Practice Lab10 (cont.): + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results and Clean up 01/10/2025 01/10/2025 https://www.youtube.com/@AWSStudyGroup 5 - Practice Lab19: VPC Peering + Introduction to VPC Peering + Initialize CloudFormation Templates + Create Security Groups + Create EC2 instances 02/10/2025 02/10/2025 https://www.youtube.com/@AWSStudyGroup 6 - Practice Lab19 (cont.): + Update Network ACLs + Create Peering Connection + Configure Route Tables + Enable Cross-Peer DNS + Clean up resources 03/10/2025 03/10/2025 https://www.youtube.com/@AWSStudyGroup Week 4 Achievements: Understood and set up Hybrid DNS with Route 53 Resolver:\nCreated Route 53 Outbound and Inbound Endpoints Configured Resolver Rules to forward DNS queries Connected DNS between on-premises and AWS Successfully practiced VPC Peering:\nCreated Peering Connection between two VPCs Configured Route Tables to enable connectivity Enabled Cross-Peer DNS resolution Understood how to update Network ACLs for peering Used CloudFormation:\nDeployed infrastructure from templates Understood how to manage resources using Infrastructure as Code Learned how to clean up resources after completing the lab\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During the internship, I participated in 3 events, each of which was a memorable experience with new, interesting, and useful knowledge, along with great gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in Event: Attendee\nEvent 2 Event Name: DevOps on AWS\nTime: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in Event: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar\nTime: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in Event: Attendee\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.5-ci-cd/",
	"title": "CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Overview This section guides you through setting up an automated CI/CD process for your Serverless Student Management System on AWS. You will use services like Amazon S3 to store artifacts, AWS CodeBuild to build frontend and backend applications, and AWS CodePipeline to automate the entire build, test, and deploy process. Applying CI/CD helps ensure that applications are always updated quickly, minimize manual errors, and improve the efficiency of developing and operating systems on the AWS platform.\nOverview diagram:\n1. Create S3 Bucket for Artifacts Bucket name: student-management-artifacts-{account-id}\rRegion: ap-southeast-1\rVersioning: Enabled 2. Create CodeBuild Project Go to AWS Console → CodeBuild → Create build project Frontend Build:\nProject name: student-management-frontend-build\rSource: GitLab (or GitHub)\rEnvironment:\r- Managed images\r- Operating system: Ubuntu\r- Runtime: Standard\r- Image: aws/codebuild/standard:7.0\rBuildspec: Use buildspec file buildspec-frontend.yml:\nversion: 0.2 phasing: install: runtime-versions: nodejs: 18 commands: - cd serverless-student-management-system-front-end - npm ci build: commands: - npm run build artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: serverless-student-management-system-front-end/build cache: paths: - \u0026#39;serverless-student-management-system-front-end/node_modules/**/*\u0026#39; Backend Build:\nProject name: student-management-backend-build buildspec-backend.yml:\nversion: 0.2 phasing: install: runtime-versions: java: corretto17 build: commands: - cd backend-project - mvn clean package -DskipTests artifacts: files: - backend-project/target/*.jar - appspec.yml discard-paths: no cache: paths: - \u0026#34;/root/.m2/**/*\u0026#34; 3. Create CodePipeline Go to AWS Console → CodePipeline → Create pipeline Pipeline settings:\nPipeline name: student-management-pipeline Service role: New service role Source stage:\nSource provider: GitLab (or GitHub) Repository: your-repo Branch: main Change detection: GitLab webhooks Build stage:\nBuild provider: AWS CodeBuild Project name: student-management-frontend-build Deploy stage:\nDeploy provider: Amazon S3 (for frontend artifacts) Or: AWS Amplify (if using Amplify) Summary Through this section, you have learned how to set up an automated CI/CD process for the Serverless Student Management System using AWS services such as S3, CodeBuild, and CodePipeline. You have practiced creating an S3 bucket to store artifacts, configuring build projects for the frontend and backend, as well as building a pipeline to automate the entire build, test, and deploy process. Applying CI/CD helps ensure the system is always updated, deployed quickly, minimizes manual errors, and improves software development efficiency.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Serverless Student Management System on AWS Overview Serverless Student Management Platform is a cloud-based student management solution, cost-effective and flexible scalability thanks to the use of AWS Serverless services (Lambda, DynamoDB, API gateway\u0026hellip;). The platform supports up to 50-100 students initially, with the ability to flexibly expand to 500–1000 students without major infrastructure changes.\nCore services: API Gateway (REST), Lambda handles business logic, DynamoDB stores data, Amplify (frontend React), Cognito (user authentication), CloudWatch (monitoring), combined with modern DevOps CI/CD processes.\nContents System Overview and Architecture Environment Preparation \u0026amp; AWS Account Setup Backend Deployment: DynamoDB, Lambda, API Gateway, Cognito Frontend Development: Amplify, Route 53, CloudFront, WAF CI/CD_Pipeline CloudWatch Clean up Experience Objectives Understand and implement multi-tier serverless architecture on AWS, operate with key services. Master authorization and user authentication with Cognito. Practice building CRUD functions. Experience modern DevOps process: automate build, test, deploy through CI/CD pipeline. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand and implement AWS Transit Gateway to connect multiple VPCs. Study the theory of Amazon EC2. Master the concepts of Instance Types, AMI, EBS, User Data. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Practice Lab20: AWS Transit Gateway + Introduction to Transit Gateway + Preparation steps + Create Transit Gateway 06/10/2025 06/10/2025 https://www.youtube.com/@AWSStudyGroup 3 - Practice Lab20 (cont.): + Create Transit Gateway Attachments + Create Transit Gateway Route Tables 07/10/2025 07/10/2025 https://www.youtube.com/@AWSStudyGroup 4 - Practice Lab20 (cont.): + Add Transit Gateway Routes to VPC Route Tables + Test connectivity between VPCs + Clean up resources 08/10/2025 08/10/2025 https://www.youtube.com/@AWSStudyGroup 5 - Study Theory Module 03-01: Amazon EC2 + Compute VM on AWS + Instance Types (General, Compute, Memory, Storage, Accelerated) + AMI (Amazon Machine Image) + Backup and Key Pair 09/10/2025 09/10/2025 https://www.youtube.com/@AWSStudyGroup 6 - Study Theory Module 03-01 (cont.): + Elastic Block Store (EBS) + Instance Store + User Data + Metadata + EC2 Auto Scaling 10/10/2025 10/10/2025 https://www.youtube.com/@AWSStudyGroup Week 5 Achievements: Successfully practiced AWS Transit Gateway:\nCreated Transit Gateway to connect multiple VPCs Created Transit Gateway Attachments for each VPC Configured Transit Gateway Route Tables Updated VPC Route Tables to use Transit Gateway Tested connectivity between VPCs via Transit Gateway Compared Transit Gateway vs VPC Peering:\nTransit Gateway: Hub-and-spoke model, scalable, centralized VPC Peering: Point-to-point, not transitive Mastered Amazon EC2 theory:\nTypes of Instance Types and use cases What is AMI and how to create custom AMI EBS volumes and types (gp2, gp3, io1, io2, st1, sc1) Instance Store vs EBS User Data to bootstrap EC2 Metadata service to get instance information EC2 Auto Scaling concepts "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.6-cloud-watch/",
	"title": "Cloud Watch",
	"tags": [],
	"description": "",
	"content": "Overview This section guides you through using Amazon CloudWatch to monitor, alert, and manage services in the Serverless Student Management System on AWS. You will learn how to create log groups for Lambda, set up alarms for Lambda errors and performance, API Gateway, and build a visual dashboard to track the entire system\u0026rsquo;s operations. Proactive monitoring helps detect problems early, optimize operations, and effectively control AWS costs.\nCloudWatch (Monitoring) 1. Create Log Groups Lambda automatically creates log group: /aws/lambda/student-management-api\n2. Create Alarms Go to CloudWatch → Alarms → Create alarm Lambda Error Alarm:\nMetric: AWS/Lambda → Errors\rFunction: student-management-api\rStatistics: Sum\rPeriod: 5 minutes\rThreshold: \u0026gt; 5 API Gateway 5xx Alarm:\nMetric: AWS/ApiGateway → 5XXError\rStatistics: Sum\rPeriod: 5 minutes\rThreshold: \u0026gt; 10 3. Create Dashboard Go to CloudWatch → Dashboards → Create dashboard Add widgets: Lambda invocations Lambda errors Lambda duration API Gateway requests DynamoDB read/write capacity Summary Through this section, you have learned how to use Amazon CloudWatch to monitor, alert, and effectively manage AWS services in a serverless system. Setting up log groups, alarms, and dashboards helps you detect problems early, optimize operations, control costs, and ensure the system always operates stably and securely. This is an important step to maintain service quality and support long-term operations on the AWS platform.\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/6-self-evaluation/",
	"title": "Self-evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS from 09/08/2025 to 12/09/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI participated in the Serverless Student Management System project as the Back End developer, through which I developed and improved skills such as programming, requirements analysis, architecture design, event handling, report writing, and technical communication with the team.\nRegarding work ethic, I always strive to complete assigned tasks, comply with regulations, and proactively communicate with colleagues to improve work efficiency.\nAchievements Gained an understanding of multi-layer serverless architecture operating on AWS. Understood the Cognito authorization model, and real-time processing via AppSync WebSocket. Directly developed modules for CRUD, chat, student ranking, and event-based email sending. Practiced DevOps CI/CD pipeline automating the build → test → deploy process. Self-Evaluation Table No. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, work quality ☐ ✅ ☐ 2 Ability to learn Quickly absorb new knowledge, self-improvement ☐ ✅ ☐ 3 Proactiveness Self-learning, taking on tasks without being asked ☐ ✅ ☐ 4 Sense of responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Compliance with time, processes, company regulations ☐ ✅ ☐ 6 Progressiveness Receiving feedback and continuously improving ☐ ☐ ✅ 7 Communication Reporting work, presenting ideas ☐ ☐ ✅ 8 Teamwork Working effectively with the team, proactively supporting ✅ ☐ ☐ 9 Professional conduct Serious attitude, respect for colleagues, professionalism ✅ ☐ ☐ 10 Problem-solving thinking Identifying problems, proposing solutions ☐ ☐ ✅ 11 Contribution to project/org Work results, level of contribution, improvements ☐ ✅ ☐ 12 Overall Overall evaluation of the entire internship process ☐ ✅ ☐ Areas for Improvement Improve discipline, better compliance with rules and time management. Enhance analytical \u0026amp; problem-solving thinking in a more systematic way. Practice communication skills, present more clearly in reports and work discussions. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Study the theory of EC2 Auto Scaling, EFS/FSx, Lightsail, MGN. Practice AWS Backup to backup and restore EC2. Deploy AWS Storage Gateway. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Study Theory Module 03-02: + EC2 Auto Scaling + Amazon EFS and FSx + Amazon Lightsail + AWS Migration Hub (MGN) 13/10/2025 13/10/2025 https://www.youtube.com/@AWSStudyGroup 3 - Practice Lab13: AWS Backup + Introduction to AWS Backup + Deploy infrastructure + Create Backup plan 14/10/2025 14/10/2025 https://www.youtube.com/@AWSStudyGroup 4 - Practice Lab13 (cont.): + Test Restore from backup + Clean up resources 15/10/2025 15/10/2025 https://www.youtube.com/@AWSStudyGroup 5 - Practice Lab24: AWS Storage Gateway + Create S3 Bucket + Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares 16/10/2025 16/10/2025 https://www.youtube.com/@AWSStudyGroup 6 - Practice Lab57: S3 Static Website + Create S3 bucket + Load data + Enable static website feature + Configure public access 17/10/2025 17/10/2025 https://www.youtube.com/@AWSStudyGroup Week 6 Achievements: Mastered the theory of additional Compute services:\nEC2 Auto Scaling: Launch configurations, Auto Scaling Groups Amazon EFS: Shared file storage for EC2 Amazon FSx: Managed file systems (Windows, Lustre, NetApp, OpenZFS) Amazon Lightsail: Simplified compute service AWS MGN: Application migration service Successfully practiced AWS Backup:\nCreated Backup plan with schedule Performed on-demand backup Restored EC2 from backup Understood retention policies Deployed AWS Storage Gateway:\nCreated File Gateway on EC2 Configured File Shares connected to S3 Mounted file shares on-premises Deployed S3 Static Website:\nConfigured S3 bucket for static hosting Configured public access policies Tested website accessibility "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/5-workshop/5.7-clean-up/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Congratulations on successfully deploying the entire Serverless Student Management system!\nTo avoid unnecessary charges on your AWS account, follow the Clean Up steps below in top-to-bottom order (the reverse order of creation).\n1. Delete CodePipeline \u0026amp; CodeBuild Projects Go to CodePipeline → Select the pipeline student-management-pipeline → Release change (if it’s running) → Delete Go to CodeBuild → Delete the 2 projects: student-management-frontend-build student-management-backend-build 2. Delete CloudFront Distribution Go to CloudFront → Select your distribution → Disable → Wait ~10–15 minutes → Delete (You cannot delete immediately if it’s still In Progress → you must Disable first) 3. Delete Amplify App (if using Amplify Console) Go to AWS Amplify → Select the app → Actions → Delete app → Type delete to confirm 4. Delete API Gateway Go to API Gateway → Select student-management-api (HTTP API) → Delete 5. Delete Lambda Function Go to Lambda → Select the function student-management-api → Actions → Delete 6. Delete AppSync API (if you created the real-time chat) Go to AppSync → Select student-management-chat → Delete 7. Delete Cognito User Pool Go to Cognito → User Pools → Select your pool → Delete user pool Note: If any user is still signed in, you must force delete or delete the users first.\n8. Delete DynamoDB Tables Go to DynamoDB → Tables → Delete the following 4 tables: student-management-users student-management-classes student-management-subjects student-management-notifications (If you have a Messages table for AppSync → delete it as well) 9. Delete WAF Web ACL Go to WAF \u0026amp; Shield → Web ACLs → Region Global (CloudFront) → Select student-management-waf → Delete 10. Delete Route53 Hosted Zone (if you created one) Go to Route53 → Hosted zones → Select your domain yourdomain.com → **Delete hosted zone` Only delete if you created a new hosted zone during the workshop — do NOT delete if it’s a real domain you\u0026rsquo;re using!\n11. Delete ACM Certificate (if created in us-east-1) Go to Certificate Manager → Region us-east-1 → Select the certificate → Delete 12. Delete S3 Buckets (Artifacts \u0026amp; Amplify if any) Delete the buckets: student-management-artifacts-{account-id} Amplify-generated buckets (usually named like amplify-...) Before deleting: Empty bucket → then Delete\n13. Delete CloudWatch Log Groups (optional, for account cleanliness) Go to CloudWatch → Log groups → Delete: /aws/lambda/student-management-api /aws/apigateway/... CodeBuild/CodePipeline log groups if desired 14. Delete IAM Roles/Policies (optional but recommended) Find and delete the roles you created: Lambda execution role CodePipeline/CodeBuild roles Amplify service roles (if any) After completing the steps above, your AWS account will no longer incur any charges from this workshop (except for a few remaining cents within the current billing cycle).\n"
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Study the theory of Amazon S3 and advanced features. Practice S3 CloudFront, Bucket Versioning, Cross-Region Replication. Learn about Snow Family and Storage Gateway. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Study Theory Module 04-01, 04-02: Amazon S3 + Storage services on AWS + S3 Access Points + S3 Storage Classes 20/10/2025 20/10/2025 https://www.youtube.com/@AWSStudyGroup 3 - Study Theory Module 04-03: Advanced S3 + S3 Static Website \u0026amp; CORS + Control Access (Bucket Policies, ACLs) + Object Key \u0026amp; Performance + S3 Glacier 21/10/2025 21/10/2025 https://www.youtube.com/@AWSStudyGroup 4 - Study Theory Module 04-04: + AWS Snow Family (Snowcone, Snowball, Snowmobile) + AWS Storage Gateway + AWS Backup 22/10/2025 22/10/2025 https://www.youtube.com/@AWSStudyGroup 5 - Practice Lab57 (cont.): + Configure Amazon CloudFront + Test Amazon CloudFront + Bucket Versioning + Move objects between storage classes 23/10/2025 23/10/2025 https://www.youtube.com/@AWSStudyGroup 6 - Practice Lab57 (cont.): + Replicate Object multi Region + Cross-Region Replication setup + Clean up resources 24/10/2025 24/10/2025 https://www.youtube.com/@AWSStudyGroup Week 7 Achievements: Mastered the theory of Amazon S3:\nS3 Storage Classes: Standard, IA, One Zone-IA, Glacier, Deep Archive S3 Access Points for multi-tenant access S3 Lifecycle policies for cost optimization S3 Versioning for data protection Understood S3 Security and Access Control:\nBucket Policies and IAM Policies ACLs (Access Control Lists) S3 Block Public Access Pre-signed URLs Successfully practiced:\nConfigured CloudFront distribution for S3 Enabled and used Bucket Versioning Set up Cross-Region Replication Moved objects between Storage Classes Understood AWS Snow Family:\nUse cases for offline data migration Snowcone, Snowball Edge, Snowmobile Mastered Storage Gateway:\nFile Gateway, Volume Gateway, Tape Gateway Hybrid cloud storage scenarios "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/7-feedback/",
	"title": "Feedback &amp; Suggestions",
	"tags": [],
	"description": "",
	"content": "General Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always ready to help when I encounter difficulties, even outside of working hours. The workspace is tidy and comfortable, which helps me focus better. However, I think it would be beneficial to add more events to provide additional knowledge.\n2. Support from Mentor / Team Admin\nThe mentor is very enthusiastic, always listens, and explains clearly when I raise questions. The admin team supports with procedures, documents, and creates favorable conditions for my work. I highly appreciate that the mentor allows me to try and solve problems myself instead of just giving the answers.\n3. Suitability Between Work and Major\nThe tasks assigned to me are suitable for the knowledge I learned at school, while also expanding into new areas I had not encountered before. This helped me both reinforce my foundational knowledge and learn practical skills.\n4. Opportunities for Learning \u0026amp; Skill Development\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and how to communicate professionally in a corporate environment. The mentor also shared a lot of real-world experience, helping me better orient my career.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still has fun. Everyone strives together and supports each other regardless of position. This made me feel like a part of the team, even as an intern.\nSuggestions \u0026amp; Wishes All the team members are very enthusiastic and always guide me, from registering at the office, finding cheap parking, to recommending good and affordable lunch spots. There should be a clearer schedule so that future interns can better prepare for tests and keep up with project progress. Since AWS Cloud is currently one of the most powerful tools, interning here will help you understand all aspects of AWS Cloud—not only how to use it, but also how to optimize operations and costs. "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Analyze requirements and design the architecture for the Serverless Student Management system. Design API endpoints for the Class Management module. Design DynamoDB table structures. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Analyze project requirements: + Identify actors: Lecturer, Student + List main use cases + Draw Use Case Diagram 27/10/2025 27/10/2025 Proposal Document 3 - Design system architecture: + Draw Architecture Diagram + Identify AWS services used + Design data flow 28/10/2025 28/10/2025 AWS Well-Architected Framework 4 - Design API endpoints for Class Management: + POST /lecturer/classes - Create Class + GET /lecturer/classes - List Classes + PUT /lecturer/classes/{id} - Edit Class + DELETE /lecturer/classes/{id} - Deactivate Class 29/10/2025 29/10/2025 REST API Best Practices 5 - Design DynamoDB Tables: + Table Classes: PK, SK, GSI design + Table Students: Schema and indexes + Single-table design patterns 30/10/2025 30/10/2025 DynamoDB Best Practices 6 - Design API Request/Response: + Define JSON schemas + Error handling patterns + Validation rules 31/10/2025 31/10/2025 API Gateway Request Validation Week 8 Achievements: Completed requirements analysis:\nIdentified 2 main actors: Lecturer and Student 15+ use cases for the system Completed Use Case Diagram Designed Serverless architecture:\nAPI Gateway + Lambda + DynamoDB Cognito for authentication AppSync for real-time features Designed Class Management APIs:\nPOST /lecturer/classes - Create new class (with quantity limit) GET /lecturer/classes - List lecturer\u0026rsquo;s classes PUT /lecturer/classes/{id} - Update class information DELETE /lecturer/classes/{id} - Soft delete (status: 1 → 0) Designed DynamoDB schema:\nSingle-table design for performance GSI for query patterns Status field for soft delete "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Develop Backend APIs for Class Management. Develop API to List Students in Class. Set up Lambda functions and DynamoDB. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Set up development environment: + Configure AWS SAM/CDK + Create DynamoDB tables + Set up API Gateway 03/11/2025 03/11/2025 AWS SAM Documentation 3 - Develop API Create Class: + POST /lecturer/classes + Lambda handler function + Input validation + Limit number of classes per lecturer 04/11/2025 04/11/2025 AWS Lambda Best Practices 4 - Develop API List \u0026amp; Edit Class: + GET /lecturer/classes - Query DynamoDB with GSI + PUT /lecturer/classes/{id} - Update class info + Authorization check 05/11/2025 05/11/2025 DynamoDB Query Patterns 5 - Develop API Deactivate Class: + DELETE /lecturer/classes/{id} + Soft delete: Update status from 1 → 0 + Cascade logic for related data 06/11/2025 06/11/2025 DynamoDB Update Operations 6 - Develop API List Students: + GET /lecturer/students/{class_id} + Query students by class_id + Pagination and filtering + Unit testing 07/11/2025 07/11/2025 DynamoDB Pagination Week 9 Achievements: Set up development environment:\nAWS SAM project structure DynamoDB tables: Classes, Students, Enrollments API Gateway with Cognito authorizer Completed Class Management APIs:\nPOST /lecturer/classes → Create Class (with limit) GET /lecturer/classes → List Classes PUT /lecturer/classes/{id} → Edit Class DELETE /lecturer/classes/{id} → Deactivate Class (soft delete) Completed Student API:\nGET /lecturer/students/{class_id} → List Students in Class Implementation details:\nSoft delete pattern: status field (1=active, 0=inactive) GSI for query optimization Input validation with JSON Schema Error handling and logging "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Develop Backend APIs for Assignment Management (CRUD). Develop API to Create/Update Grade (grading and editing grades). Build business logic and validation. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Design Assignment schema: + DynamoDB table structure + Relationship with Class and Student + Grade storage design 10/11/2025 10/11/2025 DynamoDB Design Patterns 3 - Develop API Create Assignment: + POST /lecturer/assignments + Lambda handler + Validate assignment data + Link to class_id 11/11/2025 11/11/2025 AWS Lambda Developer Guide 4 - Develop API List \u0026amp; Edit Assignment: + GET /lecturer/classes/{class_id}/assignments + PUT /lecturer/assignments/{id} + Query and update operations 12/11/2025 12/11/2025 API Gateway REST API 5 - Develop API Delete Assignment: + DELETE /lecturer/assignments/{id} + Cascade delete grades + Authorization check 13/11/2025 13/11/2025 DynamoDB Transactions 6 - Develop API Create/Update Grade: + POST /lecturer/assignments/{assignment_id}/update-grades + Batch update grades for multiple students + Validation score range + Unit testing 14/11/2025 14/11/2025 DynamoDB BatchWriteItem Week 10 Achievements: Designed Assignment schema:\nAssignments table with composite key Grade embedded in student-assignment relationship GSI for query by class_id Completed Assignment Management APIs:\nPOST /lecturer/assignments → Create Assignment GET /lecturer/classes/{class_id}/assignments → List Assignments PUT /lecturer/assignments/{id} → Edit Assignment DELETE /lecturer/assignments/{id} → Delete Assignment Completed Grade API:\nPOST /lecturer/assignments/{assignment_id}/update-grades → Create/Update Grade (merge grading and editing grades) Implementation details:\nBatch write for bulk grade updates Score validation (0-10 or custom range) Cascade delete when deleting assignment Audit trail for grade changes "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Develop Post/Comment API for classroom interaction. Integration Testing for all APIs. Write API Documentation (OpenAPI/Swagger). Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Design Post/Comment schema: + DynamoDB table structure + Parent-child relationship for comments + Timestamp and ordering 17/11/2025 17/11/2025 DynamoDB Hierarchical Data 3 - Develop API Create Post: + POST /classes/{class_id}/posts + Lambda handler for lecturer posts + Content validation 18/11/2025 18/11/2025 AWS Lambda Developer Guide 4 - Develop API Comment (if any): + Reply to post + Nested comments structure + Permission checking (Lecturer vs Student) 19/11/2025 19/11/2025 Amazon Cognito Authorization 5 - Integration Testing: + Test all API endpoints + End-to-end testing + Performance testing + Fix bugs 20/11/2025 20/11/2025 AWS SAM Testing 6 - API Documentation: + OpenAPI/Swagger specification + Request/Response examples + Error codes documentation + Postman collection 21/11/2025 21/11/2025 API Gateway Export Week 11 Achievements: Completed Post/Comment API:\nPOST /classes/{class_id}/posts → Create Post (by Lecturer) Summary of all completed Backend APIs:\nAPI Method Endpoint Description Create Class POST /lecturer/classes Create class (with limit) List Classes GET /lecturer/classes List classes Edit Class PUT /lecturer/classes/{id} Update class Deactivate Class DELETE /lecturer/classes/{id} Soft delete (status: 1→0) List Students GET /lecturer/students/{class_id} List students in class Create Assignment POST /lecturer/assignments Create assignment List Assignments GET /lecturer/classes/{class_id}/assignments List assignments Edit Assignment PUT /lecturer/assignments/{id} Update assignment Delete Assignment DELETE /lecturer/assignments/{id} Delete assignment Update Grades POST /lecturer/assignments/{assignment_id}/update-grades Grade/edit grades Create Post POST /classes/{class_id}/posts Create post Integration Testing completed:\nUnit tests: 95% coverage Integration tests passed API Gateway + Lambda + DynamoDB working API Documentation:\nOpenAPI 3.0 specification Postman collection export "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Deploy Backend to AWS (API Gateway + Lambda + DynamoDB). Finalize Proposal document and Architecture diagrams. Prepare Demo and Presentation. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Backend to AWS: + SAM deploy to AWS + Configure API Gateway stages + Setup Cognito User Pool + Environment variables 24/11/2025 24/11/2025 AWS SAM Deploy 3 - Configure Production: + Enable CloudWatch logging + Setup alarms and monitoring + API throttling and quotas + Security review 25/11/2025 25/11/2025 CloudWatch Monitoring 4 - Finalize Proposal document: + Update Architecture diagram + Estimate actual costs + Risk assessment + Implementation roadmap 26/11/2025 26/11/2025 Proposal Document 5 - Prepare Demo: + Test all API endpoints + Prepare demo scenarios + Record demo video (if needed) + Troubleshoot issues 27/11/2025 27/11/2025 Postman API Testing 6 - Prepare Presentation: + Slide deck + Technical deep-dive + Q\u0026amp;A preparation + Final review 28/11/2025 28/11/2025 AWS Serverless Samples Week 12 Achievements: Successfully deployed to AWS:\nAPI Gateway endpoint: https://xxx.execute-api.region.amazonaws.com/prod Lambda functions deployed and running DynamoDB tables with data Cognito authentication integrated Monitoring and Security:\nCloudWatch dashboards API Gateway throttling: 1000 req/sec Cognito JWT authentication IAM least privilege policies Summary of completed Backend APIs:\n# API Method Endpoint 1 Create Class POST /lecturer/classes 2 List Classes GET /lecturer/classes 3 Edit Class PUT /lecturer/classes/{id} 4 Deactivate Class DELETE /lecturer/classes/{id} 5 List Students GET /lecturer/students/{class_id} 6 Create Assignment POST /lecturer/assignments 7 List Assignments GET /lecturer/classes/{class_id}/assignments 8 Edit Assignment PUT /lecturer/assignments/{id} 9 Delete Assignment DELETE /lecturer/assignments/{id} 10 Update Grades POST /lecturer/assignments/{assignment_id}/update-grades 11 Create Post POST /classes/{class_id}/posts Proposal Document completed:\nArchitecture diagram updated Estimated cost: ~$7-20/month 8 main risks assessed 14-week roadmap Summary of 12-week internship:\nTuần 1-7: Học AWS fundamentals (VPC, EC2, S3, IAM, RDS, Security) Tuần 8-12: Phát triển Backend APIs cho Serverless Student Management System Hoàn thành 11 REST APIs với Lambda + DynamoDB Deploy production-ready system trên AWS "
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lamthanhphuc.github.io/AWS-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]